# Solving 2048!


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

Goals: - Use selenium to open microsoft edge and control the webpage to
play the game - Access the game info real-time - Run multiple processes
in parallel to speed up data collection - Train the model

``` python
from selenium import webdriver
from selenium.webdriver.common.by import By
from fastcore.all import *
import numpy as np
import torch
```

``` python
url = "http://home.ustc.edu.cn/~hejiyan/flxg/"
driver = webdriver.Edge()
driver.get(url)
```

``` python
tile_container = driver.find_elements(By.CLASS_NAME, 'tile')
t = tile_container[0]
```

``` python
level = int(t.get_attribute('class').split()[1].split('-')[-1])
level
```

    2

``` python
position = t.get_attribute('class').split()[2].split('-')[2:]
row, col = int(position[1])-1, int(position[0])-1
```

``` python
row, col
```

    (2, 0)

``` python
grid = [[0 for i in range(4)] for j in range(4)]
grid
```

    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]

``` python
for each in tile_container:
    level = int(each.get_attribute('class').split()[1].split('-')[-1])
    position = each.get_attribute('class').split()[2].split('-')[2:]
    row, col = int(position[1])-1, int(position[0])-1
    level = np.log2(level)
    if grid[row][col] <= level:
        grid[row][col] = level
```

``` python
grid
```

    [[0, 0, 0, 0],
     [0, 0, 0, 0],
     [np.float64(1.0), 0, 0, np.float64(1.0)],
     [0, 0, 0, 0]]

``` python
gameover = len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0
```

``` python
gameover
```

    False

### Do the game play automation

``` python
from selenium.webdriver.common.keys import Keys
import random
import time
```

``` python
keys = [Keys.ARROW_DOWN, Keys.ARROW_LEFT, Keys.ARROW_UP, Keys.ARROW_RIGHT]
```

``` python
container = driver.find_element(By.TAG_NAME, 'body')
```

``` python
time.sleep(10)
while not len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0:
    container.send_keys(random.choice(keys))
```

``` python
score = int(driver.find_element(By.CLASS_NAME, 'score-container').text)
score
```

    704

``` python
def gameplay(delay=10):
    url = "http://home.ustc.edu.cn/~hejiyan/flxg/"
    driver.get(url)
    container = driver.find_element(By.TAG_NAME, 'body')
    time.sleep(delay)
    while not len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0:
        container.send_keys(random.choice(keys))
    score = driver.find_element(By.CLASS_NAME, 'score-container').text
    if '\n' in score:
        score = score.split('\n')[0]
    score = int(score)
    print("分数：",score)
```

``` python
for i in range(10):
    gameplay(0)
```

    分数： 568
    分数： 728
    分数： 1096
    分数： 756
    分数： 808
    分数： 780
    分数： 1056
    分数： 1152
    分数： 1008
    分数： 2540

### Collect Gameplay Data

``` python
def init_grid():
    return [[0.0 for i in range(4)] for j in range(4)]
def update_tiles():
    grid = init_grid()
    tc = driver.find_elements(By.CLASS_NAME, 'tile')
    for each in tc:
        cls = each.get_attribute('class')
        level = int(cls.split()[1].split('-')[-1])
        position = cls.split()[2].split('-')[2:]
        row, col = int(position[1])-1, int(position[0])-1
        level = float(np.log2(level))
        if grid[row][col] <= level:
            grid[row][col] = level
    return grid.copy()
def is_game_over():
    return len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0
def get_score():
    score = driver.find_element(By.CLASS_NAME, 'score-container').text
    if '\n' in score:
        score = score.split('\n')[0]
    score = int(score)
    return score
def refresh(driver):
    driver.get(url)
```

``` python
def random_key():
    return random.choice(keys)
def send_random_key():
    key = random_key()
    container = driver.find_element(By.TAG_NAME, 'body')
    container.send_keys(key)
    return keys.index(key)
def send_key(index):
    container = driver.find_element(By.TAG_NAME, 'body')
    container.send_keys(keys[index])
```

``` python
refresh(driver)
states = []
actions = []
while not is_game_over():
    states.append(update_tiles())
    actions.append(send_random_key())
    time.sleep(0.02)
print(get_score())
```

    1112

``` python
len(states)
```

    151

``` python
actions[-10:]
```

    [0, 2, 0, 2, 3, 1, 0, 2, 2, 1]

### Implementing a neural network

``` python
model = nn.Sequential(
    nn.Linear(16, 128),
    nn.ReLU(),
    nn.Linear(128, 108),
    nn.ReLU(),
    nn.Linear(108, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 8),
    nn.ReLU(),
    nn.Linear(8, 4)
)
```

``` python
def flatten(lst):
     return torch.tensor(lst).view(1, 16)
```

``` python
F.softmax(model(flatten(states[-1])), dim=1)
```

    tensor([[0.2226, 0.2244, 0.2841, 0.2688]], grad_fn=<SoftmaxBackward0>)

``` python
scores = []
all_states = []
all_actions = []
for i in range(10):
    refresh(driver)
    states = []
    actions = []
    while not is_game_over():
        states.append(update_tiles())
        actions.append(send_random_key())
        time.sleep(0.02)
    scores.append(get_score())
    all_states.append(states)
    all_actions.append(actions)
```

``` python
scores
```

    [1424, 636, 488, 1868, 732, 1012, 1336, 1400, 1164, 696]

``` python
X = []
y = []
targ = np.percentile(scores, 50)
```

``` python
for i in range(10):
    if scores[i] > targ:
        X.extend(all_states[i])
        y.extend(all_actions[i])
```

``` python
X_train = torch.tensor(X).view(-1, 16)
X_train, X_train.shape
```

    (tensor([[0., 0., 1.,  ..., 1., 0., 0.],
             [0., 0., 1.,  ..., 0., 0., 1.],
             [0., 0., 0.,  ..., 0., 1., 2.],
             ...,
             [1., 2., 3.,  ..., 7., 1., 2.],
             [1., 2., 1.,  ..., 7., 1., 2.],
             [2., 1., 2.,  ..., 7., 1., 2.]]),
     torch.Size([914, 16]))

``` python
y_train = torch.tensor(y).view(-1, 1)
y_train[:10], y_train.shape
```

    (tensor([[3],
             [0],
             [2],
             [3],
             [2],
             [1],
             [1],
             [3],
             [2],
             [1]]),
     torch.Size([914, 1]))

``` python
optimizer = optim.Adam(model.parameters(), lr=0.01)
def nll_loss(pred, goal):
    return torch.mean((pred-goal)**2)
```

``` python
loss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))
loss.backward()
```

``` python
model(X_train)
```

    tensor([[-0.2140,  0.1534,  0.0546,  0.0998],
            [-0.2061,  0.1558,  0.0597,  0.1094],
            [-0.2398,  0.1856,  0.0360,  0.0989],
            ...,
            [-0.1910, -0.0025,  0.0620,  0.0805],
            [-0.2252,  0.0265,  0.0468,  0.0666],
            [-0.2122,  0.0110,  0.0648,  0.0661]], grad_fn=<AddmmBackward0>)

``` python
optimizer.step()
```

``` python
from torch.distributions import Categorical
```

``` python
probs = F.softmax(model(flatten(states[-1])), dim=1)
m = Categorical(probs)
m.sample()
```

    tensor([2])

``` python
probs
```

    tensor([[0.2062, 0.2503, 0.2737, 0.2699]], grad_fn=<SoftmaxBackward0>)

### Implement one complete training loop

``` python
goats = []
scores = []
all_states = []
all_actions = []
for i in range(10):
    refresh(driver)
    states = []
    actions = []
    data = []
    while not is_game_over():
        states.append(flatten(update_tiles()))
        probs = F.softmax(model(states[-1]), dim=1)
        m = Categorical(probs)
        action = m.sample()
        send_key(action)
        actions.append(action)
        data.append(states[-1], action)
        time.sleep(0.03)

    scores.append(get_score())
    if len(goats) < 5:
        goats.append((states, actions, scores[-1]))
    else:
        goats.sort(key=lambda x: x[-1])
        if goats[0][-1] < scores[-1]:
            goats[0] = (states, actions, scores[-1])
    all_states.append(states)
    all_actions.append(actions)
```

    TypeError: key_func() missing 1 required positional argument: 'b'
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mTypeError[0m                                 Traceback (most recent call last)
    Cell [0;32mIn[243], line 24[0m
    [1;32m     22[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mkey_func[39m(a, b):
    [1;32m     23[0m     [38;5;28;01mreturn[39;00m a[[38;5;241m-[39m[38;5;241m1[39m] [38;5;241m<[39m b[[38;5;241m-[39m[38;5;241m1[39m]
    [0;32m---> 24[0m [43mgoats[49m[38;5;241;43m.[39;49m[43msort[49m[43m([49m[43mkey[49m[38;5;241;43m=[39;49m[43mkey_func[49m[43m)[49m
    [1;32m     25[0m [38;5;28;01mif[39;00m goats[[38;5;241m0[39m][[38;5;241m-[39m[38;5;241m1[39m] [38;5;241m<[39m scores[[38;5;241m-[39m[38;5;241m1[39m]:
    [1;32m     26[0m     goats[[38;5;241m0[39m] [38;5;241m=[39m (states, actions, scores[[38;5;241m-[39m[38;5;241m1[39m])

    [0;31mTypeError[0m: key_func() missing 1 required positional argument: 'b'

``` python
torch.tensor(scores, dtype=float).mean()
```

    tensor(802., dtype=torch.float64)

``` python
X = []
y = []
for i in range(10):
    if scores[i] > targ:
        X.extend(all_states[i])
        y.extend(all_actions[i])
X_train = torch.tensor(X).view(-1, 16)
y_train = torch.tensor(y).view(-1, 1)
for i in range(5):
    loss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))
    loss.backward()
    optimizer.step()
```

``` python
import time
```

``` python
goats = []
```

### Train for several hours with logging!

``` python
t0 = time.time()
for k in range(30):
    # 30 epochs, with 10 games per epoch
    print(f"\n{k}th epoch. ")
    t1 = time.time()
    scores = []
    all_states = []
    all_actions = []
    for j in range(10):
        refresh(driver)
        states = []
        actions = []
        while not is_game_over():
            states.append(update_tiles())
            probs = F.softmax(model(flatten(states[-1])), dim=1)
            m = Categorical(probs)
            action = m.sample()
            send_key(action)
            actions.append(action)
            time.sleep(0.03)
        scores.append(get_score())
        print(f"Completed {j}th game. Score: {scores[-1]}.")
        if len(goats) <= 10:
            goats.append((states, actions, scores[-1]))
        else:
            goats.sort(key=lambda x: x[-1])
            if goats[0][-1] < scores[-1]:
                goats[0] = (states, actions, scores[-1])
        all_states.append(states)
        all_actions.append(actions)
    print("Average score: ", torch.tensor(scores, dtype=float).mean())
    print("GOAT score that is going to be learnt: ", [i[-1] for i in goats])
    X = []
    y = []
    for i in range(10):
        X.extend(goats[i][0])
        y.extend(goats[i][1])
    X_train = torch.tensor(X).view(-1, 16)
    y_train = torch.tensor(y).view(-1, 1)
    for i in range(5):
        loss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))
        loss.backward()
        optimizer.step()
    t2 = time.time()
    print("Epoch complete. Time elapsed: ", t2-t1, "s. Total: ", t2-t0, "s.")
```


    0th epoch. 
    Completed 0th game. Score: 1428.
    Completed 1th game. Score: 840.
    Completed 2th game. Score: 1416.
    Completed 3th game. Score: 1124.
    Completed 4th game. Score: 900.
    Completed 5th game. Score: 880.
    Completed 6th game. Score: 1116.
    Completed 7th game. Score: 956.
    Completed 8th game. Score: 872.
    Completed 9th game. Score: 560.
    Average score:  tensor(1009.2000, dtype=torch.float64)
    GOAT score that is going to be learnt:  [2368, 2372, 2380, 2380, 2384, 2392, 2416, 2428, 2824, 2952, 3068]
    Epoch complete. Time elapsed:  88.53088998794556 s. Total:  88.53138303756714 s.

    1th epoch. 

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[271], line 20[0m
    [1;32m     18[0m     send_key(action)
    [1;32m     19[0m     actions[38;5;241m.[39mappend(action)
    [0;32m---> 20[0m     [43mtime[49m[38;5;241;43m.[39;49m[43msleep[49m[43m([49m[38;5;241;43m0.03[39;49m[43m)[49m
    [1;32m     21[0m scores[38;5;241m.[39mappend(get_score())
    [1;32m     22[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m"[39m[38;5;124mCompleted [39m[38;5;132;01m{[39;00mj[38;5;132;01m}[39;00m[38;5;124mth game. Score: [39m[38;5;132;01m{[39;00mscores[[38;5;241m-[39m[38;5;241m1[39m][38;5;132;01m}[39;00m[38;5;124m.[39m[38;5;124m"[39m)

    [0;31mKeyboardInterrupt[0m: 

``` python
model(flatten(states[176]))
```

    tensor([[ 2.3761, -1.9841,  1.9022, -4.5854]], grad_fn=<AddmmBackward0>)

``` python
refresh(driver)
states = []
actions = []
while not is_game_over():
    states.append(update_tiles())
    probs = F.softmax(model(flatten(states[-1])), dim=1)
    m = Categorical(probs)
    action = m.sample()
    send_key(action)
    actions.append(action)
    time.sleep(1)
print(get_score())
```

### Localize

``` python
a = np.array([[1],[ 2], [3]])
b = a.copy()
```

``` python
b[0][0]=3
```

``` python
a==b
```

    array([[False],
           [ True],
           [ True]])

``` python
import numpy as np
import random
class Game2048:
    def __init__(self):
        self.size = 4
        self.score = 0
        self.board = np.zeros((self.size, self.size), dtype=int)
        self.add_new_tile()
        self.add_new_tile()
        self.nomove = False
    def add_new_tile(self):
        empty_tiles = list(zip(*np.where(self.board == 0)))
        if empty_tiles:
            x, y = random.choice(empty_tiles)
            self.board[x][y] = 2 if random.random() < 0.9 else 4
    def move(self, direction):
        self.nomove = False
        prev_board = self.board.copy()
        if direction == 'down':
            self.board = np.rot90(self.board, -1)
            self._move_left()
            self.board = np.rot90(self.board)
        elif direction == 'up':
            self.board = np.rot90(self.board, 1)
            self._move_left()
            self.board = np.rot90(self.board, -1)
        elif direction == 'left':
            self._move_left()
        elif direction == 'right':
            self.board = np.fliplr(self.board)
            self._move_left()
            self.board = np.fliplr(self.board)
        self.nomove = np.abs(self.board - prev_board).sum()==0
        if self.nomove: return
        self.add_new_tile()
    def _move_left(self):
        self.reward = 0
        new_board = np.zeros((self.size, self.size), dtype=int)
        for i in range(self.size):
            row = self.board[i][self.board[i] != 0]
            new_row = []
            skip = False
            for j in range(len(row)):
                if skip:
                    skip = False
                    continue
                if j + 1 < len(row) and row[j] == row[j + 1]:
                    new_row.append(row[j] * 2)
                    self.score += row[j]*2
                    self.reward += row[j]*2
                    skip = True
                else:
                    new_row.append(row[j])
            new_board[i, :len(new_row)] = new_row
        self.board[:] = new_board
        return reward
    def is_game_over(self):
        if not np.any(self.board == 0):
            for i in range(self.size):
                for j in range(self.size - 1):
                    if self.board[i][j] == self.board[i][j + 1] or \
                        self.board[j][i] == self.board[j + 1][i]:
                        return False
            return True
        return False
game = Game2048()
print(game.board)
```

    [[0 0 0 0]
     [0 0 0 0]
     [0 0 0 0]
     [0 2 2 0]]

``` python
game.move('down')
game.board
```

    array([[0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 4, 4]])

``` python
game.board
```

    array([[0, 0, 4, 2],
           [0, 0, 0, 0],
           [0, 0, 0, 0],
           [0, 0, 0, 0]])

``` python
model = nn.Sequential(
    nn.Linear(16, 128),
    nn.ReLU(),
    nn.Linear(128, 108),
    nn.ReLU(),
    nn.Linear(108, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 8),
    nn.ReLU(),
    nn.Linear(8, 4)
)
```

``` python
states[-1]
```

    array([[0, 0, 0, 4],
           [0, 0, 0, 2],
           [0, 0, 0, 0],
           [0, 0, 0, 0]])

``` python
game.is_game_over()
```

    False

``` python
plt.ion()
goats = []
model = nn.Sequential(
    nn.Linear(16, 128),
    nn.ReLU(),
    nn.Linear(128, 108),
    nn.ReLU(),
    nn.Linear(108, 84),
    nn.ReLU(),
    nn.Linear(84, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 8),
    nn.ReLU(),
    nn.Linear(8, 4)
)
optimizer = optim.Adam(model.parameters(), lr=0.01)
t0 = time.time()
epoch_num = []
epoch_score = []
epoch_var = []
for k in range(3000):
    # 30 epochs, with 10 games per epoch
    print(f"{k}th epoch. ")
    t1 = time.time()
    scores = []
    all_states = []
    all_actions = []
    for j in range(500):
        game = Game2048()
        states = []
        actions = []
        while not game.is_game_over():
            states.append(game.board.copy())
            probs = F.softmax(model(flatten(states[-1]).float()), dim=1)
            m = Categorical(probs)
            action = m.sample()
            action_str = ['down', 'left', 'up', 'right'][action]
            game.move(action_str)
            if game.nomove:
                action_str = random.choice(['down', 'left', 'up', 'right'])
                game.move(action_str)
            actions.append(action)
        scores.append(game.score)
        
        all_states.append(states)
        all_actions.append(actions)
        
    elite = np.percentile(scores, 80)
    avg = float(torch.tensor(scores, dtype=float).mean())
    var = float(torch.tensor(scores, dtype=float).std())
    print("Average score: ", avg)
    epoch_num.append(k)
    epoch_score.append(avg)
    epoch_var.append(var)
    clear_output(wait=True)
    plt.plot(epoch_num, epoch_score)
    plt.plot(epoch_num, epoch_var)
    plt.show()
    X = []
    y = []
    for i in range(100):
        if scores[i] > elite:
            X.extend(states)
            y.extend(actions)
    X_train = torch.tensor(X).view(-1, 16)
    y_train = torch.tensor(y).view(-1, 1)
    loss = torch.mean(-torch.log(F.softmax(model(X_train.float()), dim=1).gather(1, y_train)[:5]))
    loss.backward()
    optimizer.step()
    t2 = time.time()
```

![](00_core_files/figure-commonmark/cell-59-output-1.png)

    220th epoch. 

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[405], line 37[0m
    [1;32m     35[0m states[38;5;241m.[39mappend(game[38;5;241m.[39mboard[38;5;241m.[39mcopy())
    [1;32m     36[0m probs [38;5;241m=[39m F[38;5;241m.[39msoftmax(model(flatten(states[[38;5;241m-[39m[38;5;241m1[39m])[38;5;241m.[39mfloat()), dim[38;5;241m=[39m[38;5;241m1[39m)
    [0;32m---> 37[0m m [38;5;241m=[39m [43mCategorical[49m[43m([49m[43mprobs[49m[43m)[49m
    [1;32m     38[0m action [38;5;241m=[39m m[38;5;241m.[39msample()
    [1;32m     39[0m action_str [38;5;241m=[39m [[38;5;124m'[39m[38;5;124mdown[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mleft[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mup[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mright[39m[38;5;124m'[39m][action]

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/categorical.py:72[0m, in [0;36mCategorical.__init__[0;34m(self, probs, logits, validate_args)[0m
    [1;32m     68[0m [38;5;28mself[39m[38;5;241m.[39m_num_events [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39msize()[[38;5;241m-[39m[38;5;241m1[39m]
    [1;32m     69[0m batch_shape [38;5;241m=[39m (
    [1;32m     70[0m     [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39msize()[:[38;5;241m-[39m[38;5;241m1[39m] [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39mndimension() [38;5;241m>[39m [38;5;241m1[39m [38;5;28;01melse[39;00m torch[38;5;241m.[39mSize()
    [1;32m     71[0m )
    [0;32m---> 72[0m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__init__[39;49m[43m([49m[43mbatch_shape[49m[43m,[49m[43m [49m[43mvalidate_args[49m[38;5;241;43m=[39;49m[43mvalidate_args[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/distribution.py:69[0m, in [0;36mDistribution.__init__[0;34m(self, batch_shape, event_shape, validate_args)[0m
    [1;32m     67[0m     [38;5;28;01mcontinue[39;00m  [38;5;66;03m# skip checking lazily-constructed args[39;00m
    [1;32m     68[0m value [38;5;241m=[39m [38;5;28mgetattr[39m([38;5;28mself[39m, param)
    [0;32m---> 69[0m valid [38;5;241m=[39m [43mconstraint[49m[38;5;241;43m.[39;49m[43mcheck[49m[43m([49m[43mvalue[49m[43m)[49m
    [1;32m     70[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m valid[38;5;241m.[39mall():
    [1;32m     71[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
    [1;32m     72[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124mExpected parameter [39m[38;5;132;01m{[39;00mparam[38;5;132;01m}[39;00m[38;5;124m [39m[38;5;124m"[39m
    [1;32m     73[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124m([39m[38;5;132;01m{[39;00m[38;5;28mtype[39m(value)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m of shape [39m[38;5;132;01m{[39;00m[38;5;28mtuple[39m(value[38;5;241m.[39mshape)[38;5;132;01m}[39;00m[38;5;124m) [39m[38;5;124m"[39m
    [0;32m   (...)[0m
    [1;32m     76[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124mbut found invalid values:[39m[38;5;130;01m\n[39;00m[38;5;132;01m{[39;00mvalue[38;5;132;01m}[39;00m[38;5;124m"[39m
    [1;32m     77[0m     )

    [0;31mKeyboardInterrupt[0m: 

``` python
np.percentile([2, 3, 5, 6, 7, 8], 70)
```

    np.float64(6.5)

``` python
import matplotlib.pyplot as plt
import numpy as np
import time
from IPython.display import clear_output

x, y = [], []
plt.ion()  # 打开交互模式

for i in range(50):
    print(i)
    x.append(i)
    y.append(np.sin(i/5))
    clear_output(wait=True)
    plt.figure(figsize=(6,4))
    plt.plot(x, y, marker='o')
    plt.title("动态绘图示例")
    plt.show()
```

![](00_core_files/figure-commonmark/cell-61-output-1.png)

``` python
X_train
```

    tensor([[ 0,  0,  2,  ...,  0,  0,  0],
            [ 4,  0,  0,  ...,  2,  0,  0],
            [ 0,  0,  0,  ...,  2,  0,  0],
            ...,
            [ 4,  2,  4,  ...,  8, 64,  2],
            [ 4,  2,  4,  ...,  8, 64,  2],
            [ 4,  2,  4,  ...,  8, 64,  2]])

### Policy Gradient

It seems that using the cross entropy method doesn’t train well.

I need to rewrite the data collection part.

``` python
model(flatten(states[1])) # 下左上右
```

    tensor([[ 0.2096, -0.5115,  0.1361, -0.7855]], grad_fn=<AddmmBackward0>)

``` python
model = nn.Sequential(
    nn.Linear(16, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 8),
    nn.ReLU(),
    nn.Linear(8, 4)
)
```

``` python
game = Game2048()
experiences = []
while not game.is_game_over():
    game_prev = flatten(game.board.copy()).float().log1p()
    #import pdb;pdb.set_trace()
    probs = F.softmax(model(game_prev), dim=1)
    m = Categorical(probs)
    action = m.sample()
    action_str = ['down', 'left', 'up', 'right'][action]
    game.move(action_str)
    reward = game.reward
    game_after = flatten(game.board.copy()).float().log1p()
    experiences.append((game_prev, action, m.log_prob(action), reward, game_after))
```

``` python
real_rewards = []
cumu = 0
discount = 0.99
for i in experiences[::-1]:
    cumu += i[3]*discount
    real_rewards.insert(0, cumu)
```

``` python
experiences[0][2]
```

    tensor([-1.3613], grad_fn=<SqueezeBackward1>)

``` python
losses = []
for i in range(len(experiences)):
    losses.append(-experiences[i][2]*real_rewards[i])
```

``` python
loss = torch.stack(losses).sum()
loss.backward()
```

Putting it all together:

``` python
optimizer = optim.Adam(model.parameters(), lr=0.001)
scores = []
max_repeat = 0
for l in range(2000):
    game = Game2048()
    experiences = []
    repeat = 0
    while not game.is_game_over():
        repeat += 1
        max_repeat = max([repeat, max_repeat])
        game_prev = flatten(game.board.copy()).float().log1p()
        probs = F.softmax(model(game_prev), dim=1)
        m = Categorical(probs)
        """
        if repeat >= 2:
            action = random.choice([0,1,2,3])
            prob = torch.log(torch.tensor([0.25]))
        else:    
        """
        action = m.sample()
        prob = m.log_prob(action)
        action_str = ['down', 'left', 'up', 'right'][action]
        game.move(action_str)
        #pdb.set_trace()
        reward = game.reward
        if game.nomove:
            reward = -1000
        game_after = flatten(game.board.copy()).float().log1p()
        experiences.append((game_prev, action, prob, reward, game_after))
    scores.append(game.score)
    real_rewards = []
    cumu = 0
    discount = 1.01
    for i in experiences[::-1]:
        cumu += i[3]*discount
        real_rewards.insert(0, cumu)
    real_rewards = [r * 0.01 for r in real_rewards]
    losses = []
    for i in range(len(experiences)):
        losses.append(-experiences[i][2]*real_rewards[i])
    loss = torch.stack(losses).sum()
    loss.backward()
    optimizer.step()
    clear_output(wait=True)
    if len(scores) > 200:
        scores = scores[-200:]
    plt.plot(scores)
    plt.show()
```

![](00_core_files/figure-commonmark/cell-70-output-1.png)

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[507], line 12[0m
    [1;32m     10[0m max_repeat [38;5;241m=[39m [38;5;28mmax[39m([repeat, max_repeat])
    [1;32m     11[0m game_prev [38;5;241m=[39m flatten(game[38;5;241m.[39mboard[38;5;241m.[39mcopy())[38;5;241m.[39mfloat()[38;5;241m.[39mlog1p()
    [0;32m---> 12[0m probs [38;5;241m=[39m F[38;5;241m.[39msoftmax([43mmodel[49m[43m([49m[43mgame_prev[49m[43m)[49m, dim[38;5;241m=[39m[38;5;241m1[39m)
    [1;32m     13[0m m [38;5;241m=[39m Categorical(probs)
    [1;32m     14[0m [38;5;250m[39m[38;5;124;03m"""[39;00m
    [1;32m     15[0m [38;5;124;03mif repeat >= 2:[39;00m
    [1;32m     16[0m [38;5;124;03m    action = random.choice([0,1,2,3])[39;00m
    [1;32m     17[0m [38;5;124;03m    prob = torch.log(torch.tensor([0.25]))[39;00m
    [1;32m     18[0m [38;5;124;03melse:    [39;00m
    [1;32m     19[0m [38;5;124;03m"""[39;00m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1734[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [1;32m   1735[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1736[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1742[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [1;32m   1743[0m [38;5;66;03m# this function, and just call forward.[39;00m
    [1;32m   1744[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
    [1;32m   1745[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [1;32m   1746[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [0;32m-> 1747[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1749[0m result [38;5;241m=[39m [38;5;28;01mNone[39;00m
    [1;32m   1750[0m called_always_called_hooks [38;5;241m=[39m [38;5;28mset[39m()

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/container.py:250[0m, in [0;36mSequential.forward[0;34m(self, input)[0m
    [1;32m    248[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mforward[39m([38;5;28mself[39m, [38;5;28minput[39m):
    [1;32m    249[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m:
    [0;32m--> 250[0m         [38;5;28minput[39m [38;5;241m=[39m [43mmodule[49m[43m([49m[38;5;28;43minput[39;49m[43m)[49m
    [1;32m    251[0m     [38;5;28;01mreturn[39;00m [38;5;28minput[39m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1734[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [1;32m   1735[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1736[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1742[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [1;32m   1743[0m [38;5;66;03m# this function, and just call forward.[39;00m
    [1;32m   1744[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
    [1;32m   1745[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [1;32m   1746[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [0;32m-> 1747[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1749[0m result [38;5;241m=[39m [38;5;28;01mNone[39;00m
    [1;32m   1750[0m called_always_called_hooks [38;5;241m=[39m [38;5;28mset[39m()

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/activation.py:133[0m, in [0;36mReLU.forward[0;34m(self, input)[0m
    [1;32m    132[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mforward[39m([38;5;28mself[39m, [38;5;28minput[39m: Tensor) [38;5;241m-[39m[38;5;241m>[39m Tensor:
    [0;32m--> 133[0m     [38;5;28;01mreturn[39;00m [43mF[49m[38;5;241;43m.[39;49m[43mrelu[49m[43m([49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[43minplace[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43minplace[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/functional.py:1704[0m, in [0;36mrelu[0;34m(input, inplace)[0m
    [1;32m   1702[0m     result [38;5;241m=[39m torch[38;5;241m.[39mrelu_([38;5;28minput[39m)
    [1;32m   1703[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1704[0m     result [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mrelu[49m[43m([49m[38;5;28;43minput[39;49m[43m)[49m
    [1;32m   1705[0m [38;5;28;01mreturn[39;00m result

    [0;31mKeyboardInterrupt[0m: 

``` python
scores = 0
for i in range(50):
    game = Game2048()
    while not game.is_game_over():
        game_prev = flatten(game.board.copy()).float()
        probs = F.softmax(model(game_prev), dim=1)
        m = Categorical(probs)
        action = m.sample()
        action_str = ['down', 'left', 'up', 'right'][action]
        game.move(action_str)
        reward = game.reward
        game_after = game.board.copy()
    scores += game.score
print(scores/50)
```

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[491], line 7[0m
    [1;32m      5[0m game_prev [38;5;241m=[39m flatten(game[38;5;241m.[39mboard[38;5;241m.[39mcopy())[38;5;241m.[39mfloat()
    [1;32m      6[0m probs [38;5;241m=[39m F[38;5;241m.[39msoftmax(model(game_prev), dim[38;5;241m=[39m[38;5;241m1[39m)
    [0;32m----> 7[0m m [38;5;241m=[39m [43mCategorical[49m[43m([49m[43mprobs[49m[43m)[49m
    [1;32m      8[0m action [38;5;241m=[39m m[38;5;241m.[39msample()
    [1;32m      9[0m action_str [38;5;241m=[39m [[38;5;124m'[39m[38;5;124mdown[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mleft[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mup[39m[38;5;124m'[39m, [38;5;124m'[39m[38;5;124mright[39m[38;5;124m'[39m][action]

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/categorical.py:72[0m, in [0;36mCategorical.__init__[0;34m(self, probs, logits, validate_args)[0m
    [1;32m     68[0m [38;5;28mself[39m[38;5;241m.[39m_num_events [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39msize()[[38;5;241m-[39m[38;5;241m1[39m]
    [1;32m     69[0m batch_shape [38;5;241m=[39m (
    [1;32m     70[0m     [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39msize()[:[38;5;241m-[39m[38;5;241m1[39m] [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39m_param[38;5;241m.[39mndimension() [38;5;241m>[39m [38;5;241m1[39m [38;5;28;01melse[39;00m torch[38;5;241m.[39mSize()
    [1;32m     71[0m )
    [0;32m---> 72[0m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__init__[39;49m[43m([49m[43mbatch_shape[49m[43m,[49m[43m [49m[43mvalidate_args[49m[38;5;241;43m=[39;49m[43mvalidate_args[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/distribution.py:70[0m, in [0;36mDistribution.__init__[0;34m(self, batch_shape, event_shape, validate_args)[0m
    [1;32m     68[0m         value [38;5;241m=[39m [38;5;28mgetattr[39m([38;5;28mself[39m, param)
    [1;32m     69[0m         valid [38;5;241m=[39m constraint[38;5;241m.[39mcheck(value)
    [0;32m---> 70[0m         [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [43mvalid[49m[38;5;241;43m.[39;49m[43mall[49m[43m([49m[43m)[49m:
    [1;32m     71[0m             [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
    [1;32m     72[0m                 [38;5;124mf[39m[38;5;124m"[39m[38;5;124mExpected parameter [39m[38;5;132;01m{[39;00mparam[38;5;132;01m}[39;00m[38;5;124m [39m[38;5;124m"[39m
    [1;32m     73[0m                 [38;5;124mf[39m[38;5;124m"[39m[38;5;124m([39m[38;5;132;01m{[39;00m[38;5;28mtype[39m(value)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m of shape [39m[38;5;132;01m{[39;00m[38;5;28mtuple[39m(value[38;5;241m.[39mshape)[38;5;132;01m}[39;00m[38;5;124m) [39m[38;5;124m"[39m
    [0;32m   (...)[0m
    [1;32m     76[0m                 [38;5;124mf[39m[38;5;124m"[39m[38;5;124mbut found invalid values:[39m[38;5;130;01m\n[39;00m[38;5;132;01m{[39;00mvalue[38;5;132;01m}[39;00m[38;5;124m"[39m
    [1;32m     77[0m             )
    [1;32m     78[0m [38;5;28msuper[39m()[38;5;241m.[39m[38;5;21m__init__[39m()

    [0;31mKeyboardInterrupt[0m: 

``` python
class CNN2048(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # 4x4x1 -> 4x4x32
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 4x4x32 -> 4x4x64
        self.fc = nn.Linear(64*4*4, 4)  # Flatten to 4 actions
    
    def forward(self, x):
        x = x.view(-1, 1, 4, 4)  # Reshape to 4x4 grid
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # Flatten
        return self.fc(x)
```

``` python
model = CNN2048()
```

### Deep Q Networks

``` python
class FullyConnectedQNetwork(nn.Module):
    def __init__(self, nin=80, nout=4):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(nin, 192),
            nn.ReLU(),
            nn.Linear(192, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, nout)
        )
    def forward(self, x):
        return self.model(x)
def mse_loss(pred, goal):
    return torch.mean((pred-goal)**2)
```

``` python
model = FullyConnectedQNetwork()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

``` python
def get_state(game):
    states = []
    for move in ['down', 'left', 'up', 'right']:
        new_game = Game2048()
        new_game.board = game.board.copy()
        new_game.move(move)
        states.append(flatten(new_game.board.copy()).float().log1p())  
    state = flatten(game.board.copy()).float().log1p()
    states = [state] + states
    new_state = torch.cat(states, dim=1)
    return new_state
```

``` python
total_reward = 0
done = False
game = Game2048()

gamma = 0.99
experience_pool = []
while not game.is_game_over():
    state = get_state(game)
    if np.random.rand() > 0.2:
        action = int(model(state).argmax())
    else:
        action = random.randint(0, 3)
    game.move(['down', 'left', 'up', 'right'][action])
    next_state = get_state(game)
    if game.nomove:
        reward = -100
    else:
        reward = game.reward
    with torch.no_grad():
        target = reward + gamma*model(next_state).max()*(not game.is_game_over())
    experience_pool.append((state, action, target))
```

    RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x80 and 272x324)
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mRuntimeError[0m                              Traceback (most recent call last)
    Cell [0;32mIn[588], line 10[0m
    [1;32m      8[0m state [38;5;241m=[39m get_state(game)
    [1;32m      9[0m [38;5;28;01mif[39;00m np[38;5;241m.[39mrandom[38;5;241m.[39mrand() [38;5;241m>[39m [38;5;241m0.2[39m:
    [0;32m---> 10[0m     action [38;5;241m=[39m [38;5;28mint[39m([43mmodel[49m[43m([49m[43mstate[49m[43m)[49m[38;5;241m.[39margmax())
    [1;32m     11[0m [38;5;28;01melse[39;00m:
    [1;32m     12[0m     action [38;5;241m=[39m random[38;5;241m.[39mrandint([38;5;241m0[39m, [38;5;241m3[39m)

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1734[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [1;32m   1735[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1736[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1742[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [1;32m   1743[0m [38;5;66;03m# this function, and just call forward.[39;00m
    [1;32m   1744[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
    [1;32m   1745[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [1;32m   1746[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [0;32m-> 1747[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1749[0m result [38;5;241m=[39m [38;5;28;01mNone[39;00m
    [1;32m   1750[0m called_always_called_hooks [38;5;241m=[39m [38;5;28mset[39m()

    Cell [0;32mIn[578], line 16[0m, in [0;36mFullyConnectedQNetwork.forward[0;34m(self, x)[0m
    [1;32m     15[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mforward[39m([38;5;28mself[39m, x):
    [0;32m---> 16[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m[43mx[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1734[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [1;32m   1735[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1736[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1742[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [1;32m   1743[0m [38;5;66;03m# this function, and just call forward.[39;00m
    [1;32m   1744[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
    [1;32m   1745[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [1;32m   1746[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [0;32m-> 1747[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1749[0m result [38;5;241m=[39m [38;5;28;01mNone[39;00m
    [1;32m   1750[0m called_always_called_hooks [38;5;241m=[39m [38;5;28mset[39m()

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/container.py:250[0m, in [0;36mSequential.forward[0;34m(self, input)[0m
    [1;32m    248[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mforward[39m([38;5;28mself[39m, [38;5;28minput[39m):
    [1;32m    249[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m:
    [0;32m--> 250[0m         [38;5;28minput[39m [38;5;241m=[39m [43mmodule[49m[43m([49m[38;5;28;43minput[39;49m[43m)[49m
    [1;32m    251[0m     [38;5;28;01mreturn[39;00m [38;5;28minput[39m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1734[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [1;32m   1735[0m [38;5;28;01melse[39;00m:
    [0;32m-> 1736[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
    [1;32m   1742[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [1;32m   1743[0m [38;5;66;03m# this function, and just call forward.[39;00m
    [1;32m   1744[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
    [1;32m   1745[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [1;32m   1746[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [0;32m-> 1747[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1749[0m result [38;5;241m=[39m [38;5;28;01mNone[39;00m
    [1;32m   1750[0m called_always_called_hooks [38;5;241m=[39m [38;5;28mset[39m()

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/linear.py:125[0m, in [0;36mLinear.forward[0;34m(self, input)[0m
    [1;32m    124[0m [38;5;28;01mdef[39;00m[38;5;250m [39m[38;5;21mforward[39m([38;5;28mself[39m, [38;5;28minput[39m: Tensor) [38;5;241m-[39m[38;5;241m>[39m Tensor:
    [0;32m--> 125[0m     [38;5;28;01mreturn[39;00m [43mF[49m[38;5;241;43m.[39;49m[43mlinear[49m[43m([49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mweight[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbias[49m[43m)[49m

    [0;31mRuntimeError[0m: mat1 and mat2 shapes cannot be multiplied (1x80 and 272x324)

``` python
random.shuffle(experience_pool)
batch_size = 32
for k in range(0, len(experience_pool), batch_size):
    states, actions, targets = zip(*experience_pool[k:k+batch_size])
    X = torch.stack(states)
    y = torch.stack(targets)
    q_values = model(X).squeeze()
    #pdb.set_trace()
    chosen_q_values = q_values.gather(1, torch.tensor(actions).unsqueeze(1))
    loss = mse_loss(chosen_q_values.squeeze(), y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
def get_state(game):
    states = []
    for move in ['down', 'left', 'up', 'right']:
        new_game = Game2048()
        new_game.board = game.board.copy()
        new_game.move(move)
        states.append(flatten(new_game.board.copy()).float().log1p())  
    for move in ['down', 'left', 'up', 'right']:
        new_game = Game2048()
        new_game.board = game.board.copy()
        new_game.move(move)
        states.append(flatten(new_game.board.copy()).float().log1p())
    state = flatten(game.board.copy()).float().log1p()
    states = [state] + states
    new_state = torch.cat(states, dim=1)
    return new_state
```

``` python
class FullyConnectedQNetwork(nn.Module):
    def __init__(self, nin=144, nout=4):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(nin, 192),
            nn.ReLU(),
            nn.Linear(192, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, nout)
        )
    def forward(self, x):
        return self.model(x)
def mse_loss(pred, goal):
    return torch.mean((pred-goal)**2)
```

``` python
episodes = 2000
epsilons = np.geomspace(0.5, 0.1, episodes)
# model = FullyConnectedQNetwork()
# optimizer = optim.Adam(model.parameters(), lr=0.002)
gamma = 0.995
games_each_round = 15
avgs = []
stds = []
for epoch in range(episodes):
    experience_pool = []
    rewards = []
    for game_round in range(games_each_round):
        game = Game2048()
        while not game.is_game_over():
            state = get_state(game)
            if np.random.rand() > 0.05:
                action = int(model(state).argmax())
            else:
                action = random.randint(0, 3)
            game.move(['down', 'left', 'up', 'right'][action])
            next_state = get_state(game)
            if game.nomove:
                reward = -20
            else:
                reward = game.reward
            with torch.no_grad():
                target = reward + gamma*model(next_state).max()*(not game.is_game_over())
            experience_pool.append((state, action, target))
        rewards.append(game.score)
    rt = torch.tensor(rewards).float()
    avgs.append(rt.mean())
    stds.append(rt.std())
    if len(avgs)>200:
        avgs.pop(0)
        stds.pop(0)
    clear_output(wait=True)
    plt.plot(avgs)
    plt.plot(stds)
    plt.show()
    random.shuffle(experience_pool)
    batch_size = 64
    for k in range(0, len(experience_pool), batch_size):
        states, actions, targets = zip(*experience_pool[k:k+batch_size])
        X = torch.stack(states)
        y = torch.stack(targets)
        q_values = model(X).squeeze(1)
        try:
            chosen_q_values = q_values.gather(1, torch.tensor(actions).unsqueeze(1))
        except Exception:
            pdb.set_trace()
        loss = mse_loss(chosen_q_values.squeeze(), y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

![](00_core_files/figure-commonmark/cell-80-output-1.png)

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[611], line 15[0m
    [1;32m     13[0m game [38;5;241m=[39m Game2048()
    [1;32m     14[0m [38;5;28;01mwhile[39;00m [38;5;129;01mnot[39;00m game[38;5;241m.[39mis_game_over():
    [0;32m---> 15[0m     state [38;5;241m=[39m [43mget_state[49m[43m([49m[43mgame[49m[43m)[49m
    [1;32m     16[0m     [38;5;28;01mif[39;00m np[38;5;241m.[39mrandom[38;5;241m.[39mrand() [38;5;241m>[39m [38;5;241m0.05[39m:
    [1;32m     17[0m         action [38;5;241m=[39m [38;5;28mint[39m(model(state)[38;5;241m.[39margmax())

    Cell [0;32mIn[598], line 24[0m, in [0;36mget_state[0;34m(game)[0m
    [1;32m     22[0m     new_game [38;5;241m=[39m Game2048()
    [1;32m     23[0m     new_game[38;5;241m.[39mboard [38;5;241m=[39m game[38;5;241m.[39mboard[38;5;241m.[39mcopy()
    [0;32m---> 24[0m     [43mnew_game[49m[38;5;241;43m.[39;49m[43mmove[49m[43m([49m[43mmove[49m[43m)[49m
    [1;32m     25[0m     states[38;5;241m.[39mappend(flatten(new_game[38;5;241m.[39mboard[38;5;241m.[39mcopy())[38;5;241m.[39mfloat()[38;5;241m.[39mlog1p())
    [1;32m     26[0m state [38;5;241m=[39m flatten(game[38;5;241m.[39mboard[38;5;241m.[39mcopy())[38;5;241m.[39mfloat()[38;5;241m.[39mlog1p()

    Cell [0;32mIn[443], line 25[0m, in [0;36mGame2048.move[0;34m(self, direction)[0m
    [1;32m     23[0m [38;5;28;01melif[39;00m direction [38;5;241m==[39m [38;5;124m'[39m[38;5;124mup[39m[38;5;124m'[39m:
    [1;32m     24[0m     [38;5;28mself[39m[38;5;241m.[39mboard [38;5;241m=[39m np[38;5;241m.[39mrot90([38;5;28mself[39m[38;5;241m.[39mboard, [38;5;241m1[39m)
    [0;32m---> 25[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_move_left[49m[43m([49m[43m)[49m
    [1;32m     26[0m     [38;5;28mself[39m[38;5;241m.[39mboard [38;5;241m=[39m np[38;5;241m.[39mrot90([38;5;28mself[39m[38;5;241m.[39mboard, [38;5;241m-[39m[38;5;241m1[39m)
    [1;32m     27[0m [38;5;28;01melif[39;00m direction [38;5;241m==[39m [38;5;124m'[39m[38;5;124mleft[39m[38;5;124m'[39m:

    Cell [0;32mIn[443], line 43[0m, in [0;36mGame2048._move_left[0;34m(self)[0m
    [1;32m     41[0m new_row [38;5;241m=[39m []
    [1;32m     42[0m skip [38;5;241m=[39m [38;5;28;01mFalse[39;00m
    [0;32m---> 43[0m [38;5;28;01mfor[39;00m j [38;5;129;01min[39;00m [38;5;28mrange[39m([38;5;28;43mlen[39;49m[43m([49m[43mrow[49m[43m)[49m):
    [1;32m     44[0m     [38;5;28;01mif[39;00m skip:
    [1;32m     45[0m         skip [38;5;241m=[39m [38;5;28;01mFalse[39;00m

    [0;31mKeyboardInterrupt[0m: 

``` python
torch.save(model.state_dict(), "400_epoch_80_nin.pth")
```

``` python
model_load = FullyConnectedQNetwork()
```

``` python
model_load.load_state_dict(torch.load("400_epoch_80_nin.pth", weights_only=True))
```

    <All keys matched successfully>

``` python
game = Game2048()
```

``` python
model(get_state(Game2048()))
```

    tensor([[-0.1269, -0.0465,  0.0922,  0.0480]], grad_fn=<AddmmBackward0>)

### Play on browser with newest model

``` python
url = "http://home.ustc.edu.cn/~hejiyan/flxg/"
driver = webdriver.Edge()
driver.get(url)
t0 = time.time()
t1 = time.time()
scores = []
all_states = []
all_actions = []
for j in range(10):
    refresh(driver)
    time.sleep(1)
    states = []
    actions = []
    while not is_game_over():
        game = Game2048()
        grid = np.array([[int(j) for j in i] for i in update_tiles()])
        #pdb.set_trace()
        game.board = grid
        state = get_state(game)
        states.append(state)
        action = model(state).argmax()
        send_key(action)
        actions.append(action)
        time.sleep(0.15)
    scores.append(get_score())
    print(f"Completed {j}th game. Score: {scores[-1]}.")
    all_states.append(states)
    all_actions.append(actions)
print("Average score: ", torch.tensor(scores, dtype=float).mean())
X = []
y = []
t2 = time.time()
print("Epoch complete. Time elapsed: ", t2-t1, "s. Total: ", t2-t0, "s.")
```

    StaleElementReferenceException: Message: stale element reference: stale element not found in the current frame
      (Session info: MicrosoftEdge=140.0.3485.94); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception
    Stacktrace:
    0   msedgedriver                        0x00000001054ecfc0 msedgedriver + 5607360
    1   msedgedriver                        0x00000001054e4b2c msedgedriver + 5573420
    2   msedgedriver                        0x0000000104fdac20 msedgedriver + 289824
    3   msedgedriver                        0x0000000104fe05ec msedgedriver + 312812
    4   msedgedriver                        0x0000000104fe2df8 msedgedriver + 323064
    5   msedgedriver                        0x0000000105061510 msedgedriver + 840976
    6   msedgedriver                        0x00000001050608a0 msedgedriver + 837792
    7   msedgedriver                        0x00000001050182d0 msedgedriver + 541392
    8   msedgedriver                        0x00000001054ad8c8 msedgedriver + 5347528
    9   msedgedriver                        0x00000001054b1408 msedgedriver + 5362696
    10  msedgedriver                        0x000000010548db80 msedgedriver + 5217152
    11  msedgedriver                        0x00000001054b1c90 msedgedriver + 5364880
    12  msedgedriver                        0x000000010547f884 msedgedriver + 5159044
    13  msedgedriver                        0x00000001054d26a0 msedgedriver + 5498528
    14  msedgedriver                        0x00000001054d27c8 msedgedriver + 5498824
    15  msedgedriver                        0x00000001054e4728 msedgedriver + 5572392
    16  libsystem_pthread.dylib             0x00000001834efc0c _pthread_start + 136
    17  libsystem_pthread.dylib             0x00000001834eab80 thread_start + 8

    [0;31m---------------------------------------------------------------------------[0m
    [0;31mStaleElementReferenceException[0m            Traceback (most recent call last)
    Cell [0;32mIn[608], line 16[0m
    [1;32m     14[0m [38;5;28;01mwhile[39;00m [38;5;129;01mnot[39;00m is_game_over():
    [1;32m     15[0m     game [38;5;241m=[39m Game2048()
    [0;32m---> 16[0m     grid [38;5;241m=[39m np[38;5;241m.[39marray([[[38;5;28mint[39m(j) [38;5;28;01mfor[39;00m j [38;5;129;01min[39;00m i] [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m [43mupdate_tiles[49m[43m([49m[43m)[49m])
    [1;32m     17[0m     [38;5;66;03m#pdb.set_trace()[39;00m
    [1;32m     18[0m     game[38;5;241m.[39mboard [38;5;241m=[39m grid

    Cell [0;32mIn[149], line 7[0m, in [0;36mupdate_tiles[0;34m()[0m
    [1;32m      5[0m tc [38;5;241m=[39m driver[38;5;241m.[39mfind_elements(By[38;5;241m.[39mCLASS_NAME, [38;5;124m'[39m[38;5;124mtile[39m[38;5;124m'[39m)
    [1;32m      6[0m [38;5;28;01mfor[39;00m each [38;5;129;01min[39;00m tc:
    [0;32m----> 7[0m     [38;5;28mcls[39m [38;5;241m=[39m [43meach[49m[38;5;241;43m.[39;49m[43mget_attribute[49m[43m([49m[38;5;124;43m'[39;49m[38;5;124;43mclass[39;49m[38;5;124;43m'[39;49m[43m)[49m
    [1;32m      8[0m     level [38;5;241m=[39m [38;5;28mint[39m([38;5;28mcls[39m[38;5;241m.[39msplit()[[38;5;241m1[39m][38;5;241m.[39msplit([38;5;124m'[39m[38;5;124m-[39m[38;5;124m'[39m)[[38;5;241m-[39m[38;5;241m1[39m])
    [1;32m      9[0m     position [38;5;241m=[39m [38;5;28mcls[39m[38;5;241m.[39msplit()[[38;5;241m2[39m][38;5;241m.[39msplit([38;5;124m'[39m[38;5;124m-[39m[38;5;124m'[39m)[[38;5;241m2[39m:]

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:232[0m, in [0;36mWebElement.get_attribute[0;34m(self, name)[0m
    [1;32m    230[0m [38;5;28;01mif[39;00m getAttribute_js [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [1;32m    231[0m     _load_js()
    [0;32m--> 232[0m attribute_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mparent[49m[38;5;241;43m.[39;49m[43mexecute_script[49m[43m([49m
    [1;32m    233[0m [43m    [49m[38;5;124;43mf[39;49m[38;5;124;43m"[39;49m[38;5;124;43m/* getAttribute */return ([39;49m[38;5;132;43;01m{[39;49;00m[43mgetAttribute_js[49m[38;5;132;43;01m}[39;49;00m[38;5;124;43m).apply(null, arguments);[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43mname[49m
    [1;32m    234[0m [43m[49m[43m)[49m
    [1;32m    235[0m [38;5;28;01mreturn[39;00m attribute_value

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:555[0m, in [0;36mWebDriver.execute_script[0;34m(self, script, *args)[0m
    [1;32m    552[0m converted_args [38;5;241m=[39m [38;5;28mlist[39m(args)
    [1;32m    553[0m command [38;5;241m=[39m Command[38;5;241m.[39mW3C_EXECUTE_SCRIPT
    [0;32m--> 555[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mexecute[49m[43m([49m[43mcommand[49m[43m,[49m[43m [49m[43m{[49m[38;5;124;43m"[39;49m[38;5;124;43mscript[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mscript[49m[43m,[49m[43m [49m[38;5;124;43m"[39;49m[38;5;124;43margs[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mconverted_args[49m[43m}[49m[43m)[49m[[38;5;124m"[39m[38;5;124mvalue[39m[38;5;124m"[39m]

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:458[0m, in [0;36mWebDriver.execute[0;34m(self, driver_command, params)[0m
    [1;32m    455[0m response [38;5;241m=[39m cast(RemoteConnection, [38;5;28mself[39m[38;5;241m.[39mcommand_executor)[38;5;241m.[39mexecute(driver_command, params)
    [1;32m    457[0m [38;5;28;01mif[39;00m response:
    [0;32m--> 458[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43merror_handler[49m[38;5;241;43m.[39;49m[43mcheck_response[49m[43m([49m[43mresponse[49m[43m)[49m
    [1;32m    459[0m     response[[38;5;124m"[39m[38;5;124mvalue[39m[38;5;124m"[39m] [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_unwrap_value(response[38;5;241m.[39mget([38;5;124m"[39m[38;5;124mvalue[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m))
    [1;32m    460[0m     [38;5;28;01mreturn[39;00m response

    File [0;32m/opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:232[0m, in [0;36mErrorHandler.check_response[0;34m(self, response)[0m
    [1;32m    230[0m         alert_text [38;5;241m=[39m value[[38;5;124m"[39m[38;5;124malert[39m[38;5;124m"[39m][38;5;241m.[39mget([38;5;124m"[39m[38;5;124mtext[39m[38;5;124m"[39m)
    [1;32m    231[0m     [38;5;28;01mraise[39;00m exception_class(message, screen, stacktrace, alert_text)  [38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here[39;00m
    [0;32m--> 232[0m [38;5;28;01mraise[39;00m exception_class(message, screen, stacktrace)

    [0;31mStaleElementReferenceException[0m: Message: stale element reference: stale element not found in the current frame
      (Session info: MicrosoftEdge=140.0.3485.94); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception
    Stacktrace:
    0   msedgedriver                        0x00000001054ecfc0 msedgedriver + 5607360
    1   msedgedriver                        0x00000001054e4b2c msedgedriver + 5573420
    2   msedgedriver                        0x0000000104fdac20 msedgedriver + 289824
    3   msedgedriver                        0x0000000104fe05ec msedgedriver + 312812
    4   msedgedriver                        0x0000000104fe2df8 msedgedriver + 323064
    5   msedgedriver                        0x0000000105061510 msedgedriver + 840976
    6   msedgedriver                        0x00000001050608a0 msedgedriver + 837792
    7   msedgedriver                        0x00000001050182d0 msedgedriver + 541392
    8   msedgedriver                        0x00000001054ad8c8 msedgedriver + 5347528
    9   msedgedriver                        0x00000001054b1408 msedgedriver + 5362696
    10  msedgedriver                        0x000000010548db80 msedgedriver + 5217152
    11  msedgedriver                        0x00000001054b1c90 msedgedriver + 5364880
    12  msedgedriver                        0x000000010547f884 msedgedriver + 5159044
    13  msedgedriver                        0x00000001054d26a0 msedgedriver + 5498528
    14  msedgedriver                        0x00000001054d27c8 msedgedriver + 5498824
    15  msedgedriver                        0x00000001054e4728 msedgedriver + 5572392
    16  libsystem_pthread.dylib             0x00000001834efc0c _pthread_start + 136
    17  libsystem_pthread.dylib             0x00000001834eab80 thread_start + 8

``` python
1+1
```

    2
