[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Solving 2048!",
    "section": "",
    "text": "Goals: - Use selenium to open microsoft edge and control the webpage to play the game - Access the game info real-time - Run multiple processes in parallel to speed up data collection - Train the model\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom fastcore.all import *\nimport numpy as np\nimport torch\n\n\nurl = \"http://home.ustc.edu.cn/~hejiyan/flxg/\"\ndriver = webdriver.Edge()\ndriver.get(url)\n\n\ntile_container = driver.find_elements(By.CLASS_NAME, 'tile')\nt = tile_container[0]\n\n\nlevel = int(t.get_attribute('class').split()[1].split('-')[-1])\nlevel\n\n2\n\n\n\nposition = t.get_attribute('class').split()[2].split('-')[2:]\nrow, col = int(position[1])-1, int(position[0])-1\n\n\nrow, col\n\n(2, 0)\n\n\n\ngrid = [[0 for i in range(4)] for j in range(4)]\ngrid\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\nfor each in tile_container:\n    level = int(each.get_attribute('class').split()[1].split('-')[-1])\n    position = each.get_attribute('class').split()[2].split('-')[2:]\n    row, col = int(position[1])-1, int(position[0])-1\n    level = np.log2(level)\n    if grid[row][col] &lt;= level:\n        grid[row][col] = level\n\n\ngrid\n\n[[0, 0, 0, 0],\n [0, 0, 0, 0],\n [np.float64(1.0), 0, 0, np.float64(1.0)],\n [0, 0, 0, 0]]\n\n\n\ngameover = len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0\n\n\ngameover\n\nFalse\n\n\n\nDo the game play automation\n\nfrom selenium.webdriver.common.keys import Keys\nimport random\nimport time\n\n\nkeys = [Keys.ARROW_DOWN, Keys.ARROW_LEFT, Keys.ARROW_UP, Keys.ARROW_RIGHT]\n\n\ncontainer = driver.find_element(By.TAG_NAME, 'body')\n\n\ntime.sleep(10)\nwhile not len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0:\n    container.send_keys(random.choice(keys))\n\n\nscore = int(driver.find_element(By.CLASS_NAME, 'score-container').text)\nscore\n\n704\n\n\n\ndef gameplay(delay=10):\n    url = \"http://home.ustc.edu.cn/~hejiyan/flxg/\"\n    driver.get(url)\n    container = driver.find_element(By.TAG_NAME, 'body')\n    time.sleep(delay)\n    while not len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0:\n        container.send_keys(random.choice(keys))\n    score = driver.find_element(By.CLASS_NAME, 'score-container').text\n    if '\\n' in score:\n        score = score.split('\\n')[0]\n    score = int(score)\n    print(\"分数：\",score)\n\n\nfor i in range(10):\n    gameplay(0)\n\n分数： 568\n分数： 728\n分数： 1096\n分数： 756\n分数： 808\n分数： 780\n分数： 1056\n分数： 1152\n分数： 1008\n分数： 2540\n\n\n\n\nCollect Gameplay Data\n\ndef init_grid():\n    return [[0.0 for i in range(4)] for j in range(4)]\ndef update_tiles():\n    grid = init_grid()\n    tc = driver.find_elements(By.CLASS_NAME, 'tile')\n    for each in tc:\n        cls = each.get_attribute('class')\n        level = int(cls.split()[1].split('-')[-1])\n        position = cls.split()[2].split('-')[2:]\n        row, col = int(position[1])-1, int(position[0])-1\n        level = float(np.log2(level))\n        if grid[row][col] &lt;= level:\n            grid[row][col] = level\n    return grid.copy()\ndef is_game_over():\n    return len(driver.find_elements(By.CLASS_NAME, 'game-over')) != 0\ndef get_score():\n    score = driver.find_element(By.CLASS_NAME, 'score-container').text\n    if '\\n' in score:\n        score = score.split('\\n')[0]\n    score = int(score)\n    return score\ndef refresh(driver):\n    driver.get(url)\n\n\ndef random_key():\n    return random.choice(keys)\ndef send_random_key():\n    key = random_key()\n    container = driver.find_element(By.TAG_NAME, 'body')\n    container.send_keys(key)\n    return keys.index(key)\ndef send_key(index):\n    container = driver.find_element(By.TAG_NAME, 'body')\n    container.send_keys(keys[index])\n\n\nrefresh(driver)\nstates = []\nactions = []\nwhile not is_game_over():\n    states.append(update_tiles())\n    actions.append(send_random_key())\n    time.sleep(0.02)\nprint(get_score())\n\n1112\n\n\n\nlen(states)\n\n151\n\n\n\nactions[-10:]\n\n[0, 2, 0, 2, 3, 1, 0, 2, 2, 1]\n\n\n\n\nImplementing a neural network\n\nmodel = nn.Sequential(\n    nn.Linear(16, 128),\n    nn.ReLU(),\n    nn.Linear(128, 108),\n    nn.ReLU(),\n    nn.Linear(108, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 8),\n    nn.ReLU(),\n    nn.Linear(8, 4)\n)\n\n\ndef flatten(lst):\n     return torch.tensor(lst).view(1, 16)\n\n\nF.softmax(model(flatten(states[-1])), dim=1)\n\ntensor([[0.2226, 0.2244, 0.2841, 0.2688]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nscores = []\nall_states = []\nall_actions = []\nfor i in range(10):\n    refresh(driver)\n    states = []\n    actions = []\n    while not is_game_over():\n        states.append(update_tiles())\n        actions.append(send_random_key())\n        time.sleep(0.02)\n    scores.append(get_score())\n    all_states.append(states)\n    all_actions.append(actions)\n\n\nscores\n\n[1424, 636, 488, 1868, 732, 1012, 1336, 1400, 1164, 696]\n\n\n\nX = []\ny = []\ntarg = np.percentile(scores, 50)\n\n\nfor i in range(10):\n    if scores[i] &gt; targ:\n        X.extend(all_states[i])\n        y.extend(all_actions[i])\n\n\nX_train = torch.tensor(X).view(-1, 16)\nX_train, X_train.shape\n\n(tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 0., 1., 2.],\n         ...,\n         [1., 2., 3.,  ..., 7., 1., 2.],\n         [1., 2., 1.,  ..., 7., 1., 2.],\n         [2., 1., 2.,  ..., 7., 1., 2.]]),\n torch.Size([914, 16]))\n\n\n\ny_train = torch.tensor(y).view(-1, 1)\ny_train[:10], y_train.shape\n\n(tensor([[3],\n         [0],\n         [2],\n         [3],\n         [2],\n         [1],\n         [1],\n         [3],\n         [2],\n         [1]]),\n torch.Size([914, 1]))\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ndef nll_loss(pred, goal):\n    return torch.mean((pred-goal)**2)\n\n\nloss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))\nloss.backward()\n\n\nmodel(X_train)\n\ntensor([[-0.2140,  0.1534,  0.0546,  0.0998],\n        [-0.2061,  0.1558,  0.0597,  0.1094],\n        [-0.2398,  0.1856,  0.0360,  0.0989],\n        ...,\n        [-0.1910, -0.0025,  0.0620,  0.0805],\n        [-0.2252,  0.0265,  0.0468,  0.0666],\n        [-0.2122,  0.0110,  0.0648,  0.0661]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\noptimizer.step()\n\n\nfrom torch.distributions import Categorical\n\n\nprobs = F.softmax(model(flatten(states[-1])), dim=1)\nm = Categorical(probs)\nm.sample()\n\ntensor([2])\n\n\n\nprobs\n\ntensor([[0.2062, 0.2503, 0.2737, 0.2699]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\n\nImplement one complete training loop\n\ngoats = []\nscores = []\nall_states = []\nall_actions = []\nfor i in range(10):\n    refresh(driver)\n    states = []\n    actions = []\n    data = []\n    while not is_game_over():\n        states.append(flatten(update_tiles()))\n        probs = F.softmax(model(states[-1]), dim=1)\n        m = Categorical(probs)\n        action = m.sample()\n        send_key(action)\n        actions.append(action)\n        data.append(states[-1], action)\n        time.sleep(0.03)\n\n    scores.append(get_score())\n    if len(goats) &lt; 5:\n        goats.append((states, actions, scores[-1]))\n    else:\n        goats.sort(key=lambda x: x[-1])\n        if goats[0][-1] &lt; scores[-1]:\n            goats[0] = (states, actions, scores[-1])\n    all_states.append(states)\n    all_actions.append(actions)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[243], line 24\n     22 def key_func(a, b):\n     23     return a[-1] &lt; b[-1]\n---&gt; 24 goats.sort(key=key_func)\n     25 if goats[0][-1] &lt; scores[-1]:\n     26     goats[0] = (states, actions, scores[-1])\n\nTypeError: key_func() missing 1 required positional argument: 'b'\n\n\n\n\ntorch.tensor(scores, dtype=float).mean()\n\ntensor(802., dtype=torch.float64)\n\n\n\nX = []\ny = []\nfor i in range(10):\n    if scores[i] &gt; targ:\n        X.extend(all_states[i])\n        y.extend(all_actions[i])\nX_train = torch.tensor(X).view(-1, 16)\ny_train = torch.tensor(y).view(-1, 1)\nfor i in range(5):\n    loss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))\n    loss.backward()\n    optimizer.step()\n\n\nimport time\n\n\ngoats = []\n\n\n\nTrain for several hours with logging!\n\nt0 = time.time()\nfor k in range(30):\n    # 30 epochs, with 10 games per epoch\n    print(f\"\\n{k}th epoch. \")\n    t1 = time.time()\n    scores = []\n    all_states = []\n    all_actions = []\n    for j in range(10):\n        refresh(driver)\n        states = []\n        actions = []\n        while not is_game_over():\n            states.append(update_tiles())\n            probs = F.softmax(model(flatten(states[-1])), dim=1)\n            m = Categorical(probs)\n            action = m.sample()\n            send_key(action)\n            actions.append(action)\n            time.sleep(0.03)\n        scores.append(get_score())\n        print(f\"Completed {j}th game. Score: {scores[-1]}.\")\n        if len(goats) &lt;= 10:\n            goats.append((states, actions, scores[-1]))\n        else:\n            goats.sort(key=lambda x: x[-1])\n            if goats[0][-1] &lt; scores[-1]:\n                goats[0] = (states, actions, scores[-1])\n        all_states.append(states)\n        all_actions.append(actions)\n    print(\"Average score: \", torch.tensor(scores, dtype=float).mean())\n    print(\"GOAT score that is going to be learnt: \", [i[-1] for i in goats])\n    X = []\n    y = []\n    for i in range(10):\n        X.extend(goats[i][0])\n        y.extend(goats[i][1])\n    X_train = torch.tensor(X).view(-1, 16)\n    y_train = torch.tensor(y).view(-1, 1)\n    for i in range(5):\n        loss = torch.mean(-torch.log(F.softmax(model(X_train), dim=1).gather(1, y_train)[:5]))\n        loss.backward()\n        optimizer.step()\n    t2 = time.time()\n    print(\"Epoch complete. Time elapsed: \", t2-t1, \"s. Total: \", t2-t0, \"s.\")\n\n\n0th epoch. \nCompleted 0th game. Score: 1428.\nCompleted 1th game. Score: 840.\nCompleted 2th game. Score: 1416.\nCompleted 3th game. Score: 1124.\nCompleted 4th game. Score: 900.\nCompleted 5th game. Score: 880.\nCompleted 6th game. Score: 1116.\nCompleted 7th game. Score: 956.\nCompleted 8th game. Score: 872.\nCompleted 9th game. Score: 560.\nAverage score:  tensor(1009.2000, dtype=torch.float64)\nGOAT score that is going to be learnt:  [2368, 2372, 2380, 2380, 2384, 2392, 2416, 2428, 2824, 2952, 3068]\nEpoch complete. Time elapsed:  88.53088998794556 s. Total:  88.53138303756714 s.\n\n1th epoch. \n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[271], line 20\n     18     send_key(action)\n     19     actions.append(action)\n---&gt; 20     time.sleep(0.03)\n     21 scores.append(get_score())\n     22 print(f\"Completed {j}th game. Score: {scores[-1]}.\")\n\nKeyboardInterrupt: \n\n\n\n\nmodel(flatten(states[176]))\n\ntensor([[ 2.3761, -1.9841,  1.9022, -4.5854]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nrefresh(driver)\nstates = []\nactions = []\nwhile not is_game_over():\n    states.append(update_tiles())\n    probs = F.softmax(model(flatten(states[-1])), dim=1)\n    m = Categorical(probs)\n    action = m.sample()\n    send_key(action)\n    actions.append(action)\n    time.sleep(1)\nprint(get_score())\n\n\n\nLocalize\n\na = np.array([[1],[ 2], [3]])\nb = a.copy()\n\n\nb[0][0]=3\n\n\na==b\n\narray([[False],\n       [ True],\n       [ True]])\n\n\n\nimport numpy as np\nimport random\nclass Game2048:\n    def __init__(self):\n        self.size = 4\n        self.score = 0\n        self.board = np.zeros((self.size, self.size), dtype=int)\n        self.add_new_tile()\n        self.add_new_tile()\n        self.nomove = False\n    def add_new_tile(self):\n        empty_tiles = list(zip(*np.where(self.board == 0)))\n        if empty_tiles:\n            x, y = random.choice(empty_tiles)\n            self.board[x][y] = 2 if random.random() &lt; 0.9 else 4\n    def move(self, direction):\n        self.nomove = False\n        prev_board = self.board.copy()\n        if direction == 'down':\n            self.board = np.rot90(self.board, -1)\n            self._move_left()\n            self.board = np.rot90(self.board)\n        elif direction == 'up':\n            self.board = np.rot90(self.board, 1)\n            self._move_left()\n            self.board = np.rot90(self.board, -1)\n        elif direction == 'left':\n            self._move_left()\n        elif direction == 'right':\n            self.board = np.fliplr(self.board)\n            self._move_left()\n            self.board = np.fliplr(self.board)\n        self.nomove = np.abs(self.board - prev_board).sum()==0\n        if self.nomove: return\n        self.add_new_tile()\n    def _move_left(self):\n        self.reward = 0\n        new_board = np.zeros((self.size, self.size), dtype=int)\n        for i in range(self.size):\n            row = self.board[i][self.board[i] != 0]\n            new_row = []\n            skip = False\n            for j in range(len(row)):\n                if skip:\n                    skip = False\n                    continue\n                if j + 1 &lt; len(row) and row[j] == row[j + 1]:\n                    new_row.append(row[j] * 2)\n                    self.score += row[j]*2\n                    self.reward += row[j]*2\n                    skip = True\n                else:\n                    new_row.append(row[j])\n            new_board[i, :len(new_row)] = new_row\n        self.board[:] = new_board\n        return reward\n    def is_game_over(self):\n        if not np.any(self.board == 0):\n            for i in range(self.size):\n                for j in range(self.size - 1):\n                    if self.board[i][j] == self.board[i][j + 1] or \\\n                        self.board[j][i] == self.board[j + 1][i]:\n                        return False\n            return True\n        return False\ngame = Game2048()\nprint(game.board)\n\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [0 2 2 0]]\n\n\n\ngame.move('down')\ngame.board\n\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 4, 4]])\n\n\n\ngame.board\n\narray([[0, 0, 4, 2],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]])\n\n\n\nmodel = nn.Sequential(\n    nn.Linear(16, 128),\n    nn.ReLU(),\n    nn.Linear(128, 108),\n    nn.ReLU(),\n    nn.Linear(108, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 8),\n    nn.ReLU(),\n    nn.Linear(8, 4)\n)\n\n\nstates[-1]\n\narray([[0, 0, 0, 4],\n       [0, 0, 0, 2],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]])\n\n\n\ngame.is_game_over()\n\nFalse\n\n\n\nplt.ion()\ngoats = []\nmodel = nn.Sequential(\n    nn.Linear(16, 128),\n    nn.ReLU(),\n    nn.Linear(128, 108),\n    nn.ReLU(),\n    nn.Linear(108, 84),\n    nn.ReLU(),\n    nn.Linear(84, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 8),\n    nn.ReLU(),\n    nn.Linear(8, 4)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nt0 = time.time()\nepoch_num = []\nepoch_score = []\nepoch_var = []\nfor k in range(3000):\n    # 30 epochs, with 10 games per epoch\n    print(f\"{k}th epoch. \")\n    t1 = time.time()\n    scores = []\n    all_states = []\n    all_actions = []\n    for j in range(500):\n        game = Game2048()\n        states = []\n        actions = []\n        while not game.is_game_over():\n            states.append(game.board.copy())\n            probs = F.softmax(model(flatten(states[-1]).float()), dim=1)\n            m = Categorical(probs)\n            action = m.sample()\n            action_str = ['down', 'left', 'up', 'right'][action]\n            game.move(action_str)\n            if game.nomove:\n                action_str = random.choice(['down', 'left', 'up', 'right'])\n                game.move(action_str)\n            actions.append(action)\n        scores.append(game.score)\n        \n        all_states.append(states)\n        all_actions.append(actions)\n        \n    elite = np.percentile(scores, 80)\n    avg = float(torch.tensor(scores, dtype=float).mean())\n    var = float(torch.tensor(scores, dtype=float).std())\n    print(\"Average score: \", avg)\n    epoch_num.append(k)\n    epoch_score.append(avg)\n    epoch_var.append(var)\n    clear_output(wait=True)\n    plt.plot(epoch_num, epoch_score)\n    plt.plot(epoch_num, epoch_var)\n    plt.show()\n    X = []\n    y = []\n    for i in range(100):\n        if scores[i] &gt; elite:\n            X.extend(states)\n            y.extend(actions)\n    X_train = torch.tensor(X).view(-1, 16)\n    y_train = torch.tensor(y).view(-1, 1)\n    loss = torch.mean(-torch.log(F.softmax(model(X_train.float()), dim=1).gather(1, y_train)[:5]))\n    loss.backward()\n    optimizer.step()\n    t2 = time.time()\n\n\n\n\n\n\n\n\n220th epoch. \n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[405], line 37\n     35 states.append(game.board.copy())\n     36 probs = F.softmax(model(flatten(states[-1]).float()), dim=1)\n---&gt; 37 m = Categorical(probs)\n     38 action = m.sample()\n     39 action_str = ['down', 'left', 'up', 'right'][action]\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/categorical.py:72, in Categorical.__init__(self, probs, logits, validate_args)\n     68 self._num_events = self._param.size()[-1]\n     69 batch_shape = (\n     70     self._param.size()[:-1] if self._param.ndimension() &gt; 1 else torch.Size()\n     71 )\n---&gt; 72 super().__init__(batch_shape, validate_args=validate_args)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/distribution.py:69, in Distribution.__init__(self, batch_shape, event_shape, validate_args)\n     67     continue  # skip checking lazily-constructed args\n     68 value = getattr(self, param)\n---&gt; 69 valid = constraint.check(value)\n     70 if not valid.all():\n     71     raise ValueError(\n     72         f\"Expected parameter {param} \"\n     73         f\"({type(value).__name__} of shape {tuple(value.shape)}) \"\n   (...)\n     76         f\"but found invalid values:\\n{value}\"\n     77     )\n\nKeyboardInterrupt: \n\n\n\n\nnp.percentile([2, 3, 5, 6, 7, 8], 70)\n\nnp.float64(6.5)\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nfrom IPython.display import clear_output\n\nx, y = [], []\nplt.ion()  # 打开交互模式\n\nfor i in range(50):\n    print(i)\n    x.append(i)\n    y.append(np.sin(i/5))\n    clear_output(wait=True)\n    plt.figure(figsize=(6,4))\n    plt.plot(x, y, marker='o')\n    plt.title(\"动态绘图示例\")\n    plt.show()\n\n\n\n\n\n\n\n\n\nX_train\n\ntensor([[ 0,  0,  2,  ...,  0,  0,  0],\n        [ 4,  0,  0,  ...,  2,  0,  0],\n        [ 0,  0,  0,  ...,  2,  0,  0],\n        ...,\n        [ 4,  2,  4,  ...,  8, 64,  2],\n        [ 4,  2,  4,  ...,  8, 64,  2],\n        [ 4,  2,  4,  ...,  8, 64,  2]])\n\n\n\n\nPolicy Gradient\nIt seems that using the cross entropy method doesn’t train well.\nI need to rewrite the data collection part.\n\nmodel(flatten(states[1])) # 下左上右\n\ntensor([[ 0.2096, -0.5115,  0.1361, -0.7855]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nmodel = nn.Sequential(\n    nn.Linear(16, 128),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 32),\n    nn.ReLU(),\n    nn.Linear(32, 8),\n    nn.ReLU(),\n    nn.Linear(8, 4)\n)\n\n\ngame = Game2048()\nexperiences = []\nwhile not game.is_game_over():\n    game_prev = flatten(game.board.copy()).float().log1p()\n    #import pdb;pdb.set_trace()\n    probs = F.softmax(model(game_prev), dim=1)\n    m = Categorical(probs)\n    action = m.sample()\n    action_str = ['down', 'left', 'up', 'right'][action]\n    game.move(action_str)\n    reward = game.reward\n    game_after = flatten(game.board.copy()).float().log1p()\n    experiences.append((game_prev, action, m.log_prob(action), reward, game_after))\n\n\nreal_rewards = []\ncumu = 0\ndiscount = 0.99\nfor i in experiences[::-1]:\n    cumu += i[3]*discount\n    real_rewards.insert(0, cumu)\n\n\nexperiences[0][2]\n\ntensor([-1.3613], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\nlosses = []\nfor i in range(len(experiences)):\n    losses.append(-experiences[i][2]*real_rewards[i])\n\n\nloss = torch.stack(losses).sum()\nloss.backward()\n\nPutting it all together:\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscores = []\nmax_repeat = 0\nfor l in range(2000):\n    game = Game2048()\n    experiences = []\n    repeat = 0\n    while not game.is_game_over():\n        repeat += 1\n        max_repeat = max([repeat, max_repeat])\n        game_prev = flatten(game.board.copy()).float().log1p()\n        probs = F.softmax(model(game_prev), dim=1)\n        m = Categorical(probs)\n        \"\"\"\n        if repeat &gt;= 2:\n            action = random.choice([0,1,2,3])\n            prob = torch.log(torch.tensor([0.25]))\n        else:    \n        \"\"\"\n        action = m.sample()\n        prob = m.log_prob(action)\n        action_str = ['down', 'left', 'up', 'right'][action]\n        game.move(action_str)\n        #pdb.set_trace()\n        reward = game.reward\n        if game.nomove:\n            reward = -1000\n        game_after = flatten(game.board.copy()).float().log1p()\n        experiences.append((game_prev, action, prob, reward, game_after))\n    scores.append(game.score)\n    real_rewards = []\n    cumu = 0\n    discount = 1.01\n    for i in experiences[::-1]:\n        cumu += i[3]*discount\n        real_rewards.insert(0, cumu)\n    real_rewards = [r * 0.01 for r in real_rewards]\n    losses = []\n    for i in range(len(experiences)):\n        losses.append(-experiences[i][2]*real_rewards[i])\n    loss = torch.stack(losses).sum()\n    loss.backward()\n    optimizer.step()\n    clear_output(wait=True)\n    if len(scores) &gt; 200:\n        scores = scores[-200:]\n    plt.plot(scores)\n    plt.show()\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[507], line 12\n     10 max_repeat = max([repeat, max_repeat])\n     11 game_prev = flatten(game.board.copy()).float().log1p()\n---&gt; 12 probs = F.softmax(model(game_prev), dim=1)\n     13 m = Categorical(probs)\n     14 \"\"\"\n     15 if repeat &gt;= 2:\n     16     action = random.choice([0,1,2,3])\n     17     prob = torch.log(torch.tensor([0.25]))\n     18 else:    \n     19 \"\"\"\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/activation.py:133, in ReLU.forward(self, input)\n    132 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 133     return F.relu(input, inplace=self.inplace)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/functional.py:1704, in relu(input, inplace)\n   1702     result = torch.relu_(input)\n   1703 else:\n-&gt; 1704     result = torch.relu(input)\n   1705 return result\n\nKeyboardInterrupt: \n\n\n\n\nscores = 0\nfor i in range(50):\n    game = Game2048()\n    while not game.is_game_over():\n        game_prev = flatten(game.board.copy()).float()\n        probs = F.softmax(model(game_prev), dim=1)\n        m = Categorical(probs)\n        action = m.sample()\n        action_str = ['down', 'left', 'up', 'right'][action]\n        game.move(action_str)\n        reward = game.reward\n        game_after = game.board.copy()\n    scores += game.score\nprint(scores/50)\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[491], line 7\n      5 game_prev = flatten(game.board.copy()).float()\n      6 probs = F.softmax(model(game_prev), dim=1)\n----&gt; 7 m = Categorical(probs)\n      8 action = m.sample()\n      9 action_str = ['down', 'left', 'up', 'right'][action]\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/categorical.py:72, in Categorical.__init__(self, probs, logits, validate_args)\n     68 self._num_events = self._param.size()[-1]\n     69 batch_shape = (\n     70     self._param.size()[:-1] if self._param.ndimension() &gt; 1 else torch.Size()\n     71 )\n---&gt; 72 super().__init__(batch_shape, validate_args=validate_args)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/distributions/distribution.py:70, in Distribution.__init__(self, batch_shape, event_shape, validate_args)\n     68         value = getattr(self, param)\n     69         valid = constraint.check(value)\n---&gt; 70         if not valid.all():\n     71             raise ValueError(\n     72                 f\"Expected parameter {param} \"\n     73                 f\"({type(value).__name__} of shape {tuple(value.shape)}) \"\n   (...)\n     76                 f\"but found invalid values:\\n{value}\"\n     77             )\n     78 super().__init__()\n\nKeyboardInterrupt: \n\n\n\n\nclass CNN2048(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # 4x4x1 -&gt; 4x4x32\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 4x4x32 -&gt; 4x4x64\n        self.fc = nn.Linear(64*4*4, 4)  # Flatten to 4 actions\n    \n    def forward(self, x):\n        x = x.view(-1, 1, 4, 4)  # Reshape to 4x4 grid\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = x.view(x.size(0), -1)  # Flatten\n        return self.fc(x)\n\n\nmodel = CNN2048()\n\n\n\nDeep Q Networks\n\nclass FullyConnectedQNetwork(nn.Module):\n    def __init__(self, nin=80, nout=4):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(nin, 192),\n            nn.ReLU(),\n            nn.Linear(192, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, nout)\n        )\n    def forward(self, x):\n        return self.model(x)\ndef mse_loss(pred, goal):\n    return torch.mean((pred-goal)**2)\n\n\nmodel = FullyConnectedQNetwork()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\ndef get_state(game):\n    states = []\n    for move in ['down', 'left', 'up', 'right']:\n        new_game = Game2048()\n        new_game.board = game.board.copy()\n        new_game.move(move)\n        states.append(flatten(new_game.board.copy()).float().log1p())  \n    state = flatten(game.board.copy()).float().log1p()\n    states = [state] + states\n    new_state = torch.cat(states, dim=1)\n    return new_state\n\n\ntotal_reward = 0\ndone = False\ngame = Game2048()\n\ngamma = 0.99\nexperience_pool = []\nwhile not game.is_game_over():\n    state = get_state(game)\n    if np.random.rand() &gt; 0.2:\n        action = int(model(state).argmax())\n    else:\n        action = random.randint(0, 3)\n    game.move(['down', 'left', 'up', 'right'][action])\n    next_state = get_state(game)\n    if game.nomove:\n        reward = -100\n    else:\n        reward = game.reward\n    with torch.no_grad():\n        target = reward + gamma*model(next_state).max()*(not game.is_game_over())\n    experience_pool.append((state, action, target))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[588], line 10\n      8 state = get_state(game)\n      9 if np.random.rand() &gt; 0.2:\n---&gt; 10     action = int(model(state).argmax())\n     11 else:\n     12     action = random.randint(0, 3)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nCell In[578], line 16, in FullyConnectedQNetwork.forward(self, x)\n     15 def forward(self, x):\n---&gt; 16     return self.model(x)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/torch/nn/modules/linear.py:125, in Linear.forward(self, input)\n    124 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 125     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x80 and 272x324)\n\n\n\n\nrandom.shuffle(experience_pool)\nbatch_size = 32\nfor k in range(0, len(experience_pool), batch_size):\n    states, actions, targets = zip(*experience_pool[k:k+batch_size])\n    X = torch.stack(states)\n    y = torch.stack(targets)\n    q_values = model(X).squeeze()\n    #pdb.set_trace()\n    chosen_q_values = q_values.gather(1, torch.tensor(actions).unsqueeze(1))\n    loss = mse_loss(chosen_q_values.squeeze(), y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\ndef get_state(game):\n    states = []\n    for move in ['down', 'left', 'up', 'right']:\n        new_game = Game2048()\n        new_game.board = game.board.copy()\n        new_game.move(move)\n        states.append(flatten(new_game.board.copy()).float().log1p())  \n    for move in ['down', 'left', 'up', 'right']:\n        new_game = Game2048()\n        new_game.board = game.board.copy()\n        new_game.move(move)\n        states.append(flatten(new_game.board.copy()).float().log1p())\n    state = flatten(game.board.copy()).float().log1p()\n    states = [state] + states\n    new_state = torch.cat(states, dim=1)\n    return new_state\n\n\nclass FullyConnectedQNetwork(nn.Module):\n    def __init__(self, nin=144, nout=4):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(nin, 192),\n            nn.ReLU(),\n            nn.Linear(192, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, nout)\n        )\n    def forward(self, x):\n        return self.model(x)\ndef mse_loss(pred, goal):\n    return torch.mean((pred-goal)**2)\n\n\nepisodes = 2000\nepsilons = np.geomspace(0.5, 0.1, episodes)\n# model = FullyConnectedQNetwork()\n# optimizer = optim.Adam(model.parameters(), lr=0.002)\ngamma = 0.995\ngames_each_round = 15\navgs = []\nstds = []\nfor epoch in range(episodes):\n    experience_pool = []\n    rewards = []\n    for game_round in range(games_each_round):\n        game = Game2048()\n        while not game.is_game_over():\n            state = get_state(game)\n            if np.random.rand() &gt; 0.05:\n                action = int(model(state).argmax())\n            else:\n                action = random.randint(0, 3)\n            game.move(['down', 'left', 'up', 'right'][action])\n            next_state = get_state(game)\n            if game.nomove:\n                reward = -20\n            else:\n                reward = game.reward\n            with torch.no_grad():\n                target = reward + gamma*model(next_state).max()*(not game.is_game_over())\n            experience_pool.append((state, action, target))\n        rewards.append(game.score)\n    rt = torch.tensor(rewards).float()\n    avgs.append(rt.mean())\n    stds.append(rt.std())\n    if len(avgs)&gt;200:\n        avgs.pop(0)\n        stds.pop(0)\n    clear_output(wait=True)\n    plt.plot(avgs)\n    plt.plot(stds)\n    plt.show()\n    random.shuffle(experience_pool)\n    batch_size = 64\n    for k in range(0, len(experience_pool), batch_size):\n        states, actions, targets = zip(*experience_pool[k:k+batch_size])\n        X = torch.stack(states)\n        y = torch.stack(targets)\n        q_values = model(X).squeeze(1)\n        try:\n            chosen_q_values = q_values.gather(1, torch.tensor(actions).unsqueeze(1))\n        except Exception:\n            pdb.set_trace()\n        loss = mse_loss(chosen_q_values.squeeze(), y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[611], line 15\n     13 game = Game2048()\n     14 while not game.is_game_over():\n---&gt; 15     state = get_state(game)\n     16     if np.random.rand() &gt; 0.05:\n     17         action = int(model(state).argmax())\n\nCell In[598], line 24, in get_state(game)\n     22     new_game = Game2048()\n     23     new_game.board = game.board.copy()\n---&gt; 24     new_game.move(move)\n     25     states.append(flatten(new_game.board.copy()).float().log1p())\n     26 state = flatten(game.board.copy()).float().log1p()\n\nCell In[443], line 25, in Game2048.move(self, direction)\n     23 elif direction == 'up':\n     24     self.board = np.rot90(self.board, 1)\n---&gt; 25     self._move_left()\n     26     self.board = np.rot90(self.board, -1)\n     27 elif direction == 'left':\n\nCell In[443], line 43, in Game2048._move_left(self)\n     41 new_row = []\n     42 skip = False\n---&gt; 43 for j in range(len(row)):\n     44     if skip:\n     45         skip = False\n\nKeyboardInterrupt: \n\n\n\n\ntorch.save(model.state_dict(), \"400_epoch_80_nin.pth\")\n\n\nmodel_load = FullyConnectedQNetwork()\n\n\nmodel_load.load_state_dict(torch.load(\"400_epoch_80_nin.pth\", weights_only=True))\n\n&lt;All keys matched successfully&gt;\n\n\n\ngame = Game2048()\n\n\nmodel(get_state(Game2048()))\n\ntensor([[-0.1269, -0.0465,  0.0922,  0.0480]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\nPlay on browser with newest model\n\nurl = \"http://home.ustc.edu.cn/~hejiyan/flxg/\"\ndriver = webdriver.Edge()\ndriver.get(url)\nt0 = time.time()\nt1 = time.time()\nscores = []\nall_states = []\nall_actions = []\nfor j in range(10):\n    refresh(driver)\n    time.sleep(1)\n    states = []\n    actions = []\n    while not is_game_over():\n        game = Game2048()\n        grid = np.array([[int(j) for j in i] for i in update_tiles()])\n        #pdb.set_trace()\n        game.board = grid\n        state = get_state(game)\n        states.append(state)\n        action = model(state).argmax()\n        send_key(action)\n        actions.append(action)\n        time.sleep(0.15)\n    scores.append(get_score())\n    print(f\"Completed {j}th game. Score: {scores[-1]}.\")\n    all_states.append(states)\n    all_actions.append(actions)\nprint(\"Average score: \", torch.tensor(scores, dtype=float).mean())\nX = []\ny = []\nt2 = time.time()\nprint(\"Epoch complete. Time elapsed: \", t2-t1, \"s. Total: \", t2-t0, \"s.\")\n\n\n---------------------------------------------------------------------------\nStaleElementReferenceException            Traceback (most recent call last)\nCell In[608], line 16\n     14 while not is_game_over():\n     15     game = Game2048()\n---&gt; 16     grid = np.array([[int(j) for j in i] for i in update_tiles()])\n     17     #pdb.set_trace()\n     18     game.board = grid\n\nCell In[149], line 7, in update_tiles()\n      5 tc = driver.find_elements(By.CLASS_NAME, 'tile')\n      6 for each in tc:\n----&gt; 7     cls = each.get_attribute('class')\n      8     level = int(cls.split()[1].split('-')[-1])\n      9     position = cls.split()[2].split('-')[2:]\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:232, in WebElement.get_attribute(self, name)\n    230 if getAttribute_js is None:\n    231     _load_js()\n--&gt; 232 attribute_value = self.parent.execute_script(\n    233     f\"/* getAttribute */return ({getAttribute_js}).apply(null, arguments);\", self, name\n    234 )\n    235 return attribute_value\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:555, in WebDriver.execute_script(self, script, *args)\n    552 converted_args = list(args)\n    553 command = Command.W3C_EXECUTE_SCRIPT\n--&gt; 555 return self.execute(command, {\"script\": script, \"args\": converted_args})[\"value\"]\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:458, in WebDriver.execute(self, driver_command, params)\n    455 response = cast(RemoteConnection, self.command_executor).execute(driver_command, params)\n    457 if response:\n--&gt; 458     self.error_handler.check_response(response)\n    459     response[\"value\"] = self._unwrap_value(response.get(\"value\", None))\n    460     return response\n\nFile /opt/anaconda3/envs/deep/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:232, in ErrorHandler.check_response(self, response)\n    230         alert_text = value[\"alert\"].get(\"text\")\n    231     raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 232 raise exception_class(message, screen, stacktrace)\n\nStaleElementReferenceException: Message: stale element reference: stale element not found in the current frame\n  (Session info: MicrosoftEdge=140.0.3485.94); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\nStacktrace:\n0   msedgedriver                        0x00000001054ecfc0 msedgedriver + 5607360\n1   msedgedriver                        0x00000001054e4b2c msedgedriver + 5573420\n2   msedgedriver                        0x0000000104fdac20 msedgedriver + 289824\n3   msedgedriver                        0x0000000104fe05ec msedgedriver + 312812\n4   msedgedriver                        0x0000000104fe2df8 msedgedriver + 323064\n5   msedgedriver                        0x0000000105061510 msedgedriver + 840976\n6   msedgedriver                        0x00000001050608a0 msedgedriver + 837792\n7   msedgedriver                        0x00000001050182d0 msedgedriver + 541392\n8   msedgedriver                        0x00000001054ad8c8 msedgedriver + 5347528\n9   msedgedriver                        0x00000001054b1408 msedgedriver + 5362696\n10  msedgedriver                        0x000000010548db80 msedgedriver + 5217152\n11  msedgedriver                        0x00000001054b1c90 msedgedriver + 5364880\n12  msedgedriver                        0x000000010547f884 msedgedriver + 5159044\n13  msedgedriver                        0x00000001054d26a0 msedgedriver + 5498528\n14  msedgedriver                        0x00000001054d27c8 msedgedriver + 5498824\n15  msedgedriver                        0x00000001054e4728 msedgedriver + 5572392\n16  libsystem_pthread.dylib             0x00000001834efc0c _pthread_start + 136\n17  libsystem_pthread.dylib             0x00000001834eab80 thread_start + 8\n\n\n\n\n\n1+1\n\n2",
    "crumbs": [
      "Solving 2048!"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "solving-2048",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "solving-2048"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "solving-2048",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall solving_2048 in Development mode\n# make sure solving_2048 package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to solving_2048\n$ nbdev_prepare",
    "crumbs": [
      "solving-2048"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "solving-2048",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/Jason-XII/solving-2048.git\nor from conda\n$ conda install -c Jason-XII solving_2048\nor from pypi\n$ pip install solving_2048\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "solving-2048"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "solving-2048",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "solving-2048"
    ]
  },
  {
    "objectID": "01_convolution.html",
    "href": "01_convolution.html",
    "title": "solving-2048",
    "section": "",
    "text": "from fastcore.all import *\nimport numpy as np\nimport torch\nimport random\nimport time\n\n\n\nfrom torch import nn, optim\n\n\nimport torch.nn.functional as F\n\n\nimport numpy as np\nimport random\nclass Game2048:\n    def __init__(self):\n        self.size = 4\n        self.score = 0\n        self.board = np.zeros((self.size, self.size), dtype=int)\n        self.add_new_tile()\n        self.add_new_tile()\n        self.nomove = False\n    def add_new_tile(self):\n        empty_tiles = list(zip(*np.where(self.board == 0)))\n        if empty_tiles:\n            x, y = random.choice(empty_tiles)\n            self.board[x][y] = 2 if random.random() &lt; 0.9 else 4\n    def move(self, direction):\n        self.nomove = False\n        prev_board = self.board.copy()\n        if direction == 'down':\n            self.board = np.rot90(self.board, -1)\n            self._move_left()\n            self.board = np.rot90(self.board)\n        elif direction == 'up':\n            self.board = np.rot90(self.board, 1)\n            self._move_left()\n            self.board = np.rot90(self.board, -1)\n        elif direction == 'left':\n            self._move_left()\n        elif direction == 'right':\n            self.board = np.fliplr(self.board)\n            self._move_left()\n            self.board = np.fliplr(self.board)\n        self.nomove = np.abs(self.board - prev_board).sum()==0\n        if self.nomove: return\n        self.add_new_tile()\n    def _move_left(self):\n        self.reward = 0\n        new_board = np.zeros((self.size, self.size), dtype=int)\n        for i in range(self.size):\n            row = self.board[i][self.board[i] != 0]\n            new_row = []\n            skip = False\n            for j in range(len(row)):\n                if skip:\n                    skip = False\n                    continue\n                if j + 1 &lt; len(row) and row[j] == row[j + 1]:\n                    new_row.append(row[j] * 2)\n                    self.score += row[j]*2\n                    self.reward += row[j]*2\n                    skip = True\n                else:\n                    new_row.append(row[j])\n            new_board[i, :len(new_row)] = new_row\n        self.board[:] = new_board\n    def is_game_over(self):\n        if not np.any(self.board == 0):\n            for i in range(self.size):\n                for j in range(self.size - 1):\n                    if self.board[i][j] == self.board[i][j + 1] or \\\n                        self.board[j][i] == self.board[j + 1][i]:\n                        return False\n            return True\n        return False\ngame = Game2048()\nprint(game.board)\n\n[[0 0 0 0]\n [2 0 0 0]\n [0 0 0 0]\n [0 0 0 2]]\n\n\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(ConvBlock, self).__init__()\n        d = output_dim // 4\n        self.conv1 = nn.Conv2d(input_dim, d, 1, padding='same')\n        self.conv2 = nn.Conv2d(input_dim, d, 2, padding='same')\n        self.conv3 = nn.Conv2d(input_dim, d, 3, padding='same')\n        self.conv4 = nn.Conv2d(input_dim, d, 4, padding='same')\n\n    def forward(self, x):\n        x = x.to(device)\n        output1 = self.conv1(x)\n        output2 = self.conv2(x)\n        output3 = self.conv3(x)\n        output4 = self.conv4(x)\n        return torch.cat((output1, output2, output3, output4), dim=1)\n\nclass DQN(torch.nn.Module):\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.conv1 = ConvBlock(16, 2048)\n        self.conv2 = ConvBlock(2048, 2048)\n        self.conv3 = ConvBlock(2048, 2048)\n        self.dense1 = nn.Linear(2048 * 16, 1024)\n        self.dense2 = nn.Linear(1024, 4)\n    \n    def forward(self, x):\n        x = x.to(device)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = nn.Flatten()(x)\n        x = F.dropout(self.dense1(x))\n        return self.dense2(x)\n\n\ndef mse_loss(pred, goal):\n    return torch.mean((pred-goal)**2)\n\n\ndef flatten(lst):\n    return torch.tensor(lst).view(1, 4, 4)\nmapping = {2**i: i for i in range(1, 16)}\nmapping[0] = 0\ndef get_state(game):\n    states = []\n    for move in ['down', 'left', 'up', 'right']:\n        new_game = Game2048()\n        new_game.board = game.board.copy()\n        new_game.move(move)\n        states.append(flatten([[mapping[int(item)] for item in row] for row in game.board])) \n    states.append(flatten([[mapping[int(item)] for item in row] for row in game.board]))\n    data = torch.stack(states, dim=1)\n    return F.one_hot(data, 16).view(1, 80, 4, 4).float()\n\n\ndevice= 'mps'\nd = DQN()\nd.to(device)\ndata = get_state(Game2048())\n\n\ngame = Game2048()\n\n\nd(data)\n\n2.7 ms ± 758 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nd(data)\n\ntensor([[-0.0463,  0.0449, -0.0162, -0.0287]], device='mps:0',\n       grad_fn=&lt;LinearBackward0&gt;)\n\n\n\nfrom IPython.display import *\n\n\nimport matplotlib.pyplot as plt\n\n\nclear_output\n\n&lt;function IPython.core.display_functions.clear_output(wait=False)&gt;\n\n\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(ConvBlock, self).__init__()\n        d = output_dim // 4\n        self.conv1 = nn.Conv2d(input_dim, d, 1, padding='same')\n        self.conv2 = nn.Conv2d(input_dim, d, 2, padding='same')\n        self.conv3 = nn.Conv2d(input_dim, d, 3, padding='same')\n        self.conv4 = nn.Conv2d(input_dim, d, 4, padding='same')\n\n    def forward(self, x):\n        x = x.to(device)\n        output1 = self.conv1(x)\n        output2 = self.conv2(x)\n        output3 = self.conv3(x)\n        output4 = self.conv4(x)\n        return torch.cat((output1, output2, output3, output4), dim=1)\n\nclass DQN(torch.nn.Module):\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.conv1 = ConvBlock(80, 256)\n        self.conv2 = ConvBlock(256, 256)\n        self.conv3 = ConvBlock(256, 512)\n        self.dense1 = nn.Linear(512 * 16, 512)\n        self.dense2 = nn.Linear(512, 4)\n    \n    def forward(self, x):\n        x = x.to(device)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = nn.Flatten()(x)\n        x = F.dropout(self.dense1(x))\n        return self.dense2(x)\n\n\nclass FullyConnectedQNetwork(nn.Module):\n    def __init__(self, nin=80, nout=4):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(nin, 192),\n            nn.ReLU(),\n            nn.Linear(192, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, nout)\n        )\n    def forward(self, x):\n        return self.model(x)\n\n\nMust implement experience replay\n\nreplay_memory = []\nmax_length = 10000\n\n\ndevice = 'mps'\nepisodes = 30\nepsilons = np.geomspace(1, 0.01, episodes)\nmodel = DQN()\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.002)\ngamma = 0.985\ngames_each_round = 50\navgs = []\nstds = []\nfor epoch in range(episodes):\n    experience_pool = []\n    rewards = []\n    if epoch % 5 == 0:\n        experience_pool = experience_pool[5*batch_size:]\n    t0 = time.time()\n    for game_round in range(games_each_round):\n        t1 = time.time()\n        game = Game2048()\n        while not game.is_game_over():\n            state = get_state(game)\n            if np.random.rand() &gt; epsilons[epoch]:\n                action = int(model(state).argmax())\n            else:\n                action = random.randint(0, 3)\n            game.move(['down', 'left', 'up', 'right'][action])\n            next_state = get_state(game)\n            if game.nomove:\n                reward = -20\n            else:\n                reward = game.reward\n            if not game.is_game_over():\n                replay_memory.append((state, action, reward, next_state))\n            assert state.shape[1]==80\n            \"\"\"\n            with torch.no_grad():\n                target = reward + gamma*model(next_state).max()*(not game.is_game_over())\n            \n            experience_pool.append((state, action, target))\n            \"\"\"\n        avgs.append(game.score)\n        t2 = time.time()\n    clear_output(wait=True)\n    plt.plot(avgs)\n    plt.show()\n    batch_size = 8192\n    if len(replay_memory) &lt;= batch_size*3:\n        continue\n    \"\"\"\n    if len(replay_memory) &gt; max_length:\n        replay_memory.pop(0)\n    \"\"\"\n    pool = random.sample(replay_memory, batch_size)\n    states, actions, rewards, next_states = zip(*pool)\n    \n    X = torch.stack(states)\n    X = X.squeeze(1)\n    N = torch.stack(next_states).squeeze(1)\n    y = torch.tensor(rewards, device=device) + gamma*model(N).max()\n    q_values = model(X)\n    try:\n        chosen_q_values = q_values.gather(1, torch.tensor(actions).unsqueeze(1).to(device))\n    except Exception:\n        import pdb; pdb.set_trace()\n    loss = mse_loss(chosen_q_values.squeeze(), y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[145], line 23\n     21 state = get_state(game)\n     22 if np.random.rand() &gt; epsilons[epoch]:\n---&gt; 23     action = int(model(state).argmax())\n     24 else:\n     25     action = random.randint(0, 3)\n\nKeyboardInterrupt: \n\n\n\n\nrandom.sample?\n\n\nSignature: random.sample(population, k, *, counts=None)\nDocstring:\nChooses k unique random elements from a population sequence or set.\nReturns a new list containing elements from the population while\nleaving the original population unchanged.  The resulting list is\nin selection order so that all sub-slices will also be valid random\nsamples.  This allows raffle winners (the sample) to be partitioned\ninto grand prize and second place winners (the subslices).\nMembers of the population need not be hashable or unique.  If the\npopulation contains repeats, then each occurrence is a possible\nselection in the sample.\nRepeated elements can be specified one at a time or with the optional\ncounts parameter.  For example:\n    sample(['red', 'blue'], counts=[4, 2], k=5)\nis equivalent to:\n    sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\nTo choose a sample from a range of integers, use range() for the\npopulation argument.  This is especially fast and space efficient\nfor sampling from a large population:\n    sample(range(10000000), 60)\nFile:      /opt/anaconda3/envs/deep/lib/python3.10/random.py\nType:      method",
    "crumbs": [
      "Must implement experience replay"
    ]
  }
]